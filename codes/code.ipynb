{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0893d5b",
   "metadata": {},
   "source": [
    "## 1. Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "14ca39a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: numpy in ./.local/lib/python3.10/site-packages (1.23.5)\n",
      "Requirement already satisfied: pandas in ./.local/lib/python3.10/site-packages (2.1.4)\n",
      "Requirement already satisfied: scikit-learn in ./.local/lib/python3.10/site-packages (1.2.2)\n",
      "Requirement already satisfied: seaborn in ./.local/lib/python3.10/site-packages (0.13.2)\n",
      "Requirement already satisfied: matplotlib in ./.local/lib/python3.10/site-packages (3.8.2)\n",
      "Requirement already satisfied: optuna in ./.local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: torch in ./.local/lib/python3.10/site-packages (2.1.1+cpu)\n",
      "Requirement already satisfied: xgboost in ./.local/lib/python3.10/site-packages (2.1.1)\n",
      "Requirement already satisfied: catboost in ./.local/lib/python3.10/site-packages (1.2.7)\n",
      "Requirement already satisfied: lightgbm in ./.local/lib/python3.10/site-packages (4.5.0)\n",
      "Requirement already satisfied: tqdm in ./.local/lib/python3.10/site-packages (4.66.5)\n",
      "Requirement already satisfied: requests in ./.local/lib/python3.10/site-packages (2.32.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.local/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in ./.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in ./.local/lib/python3.10/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in ./.local/lib/python3.10/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./.local/lib/python3.10/site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: cycler>=0.10 in ./.local/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./.local/lib/python3.10/site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in ./.local/lib/python3.10/site-packages (from matplotlib) (1.3.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in ./.local/lib/python3.10/site-packages (from matplotlib) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in ./.local/lib/python3.10/site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: pillow>=8 in ./.local/lib/python3.10/site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in ./.local/lib/python3.10/site-packages (from matplotlib) (3.1.4)\n",
      "Requirement already satisfied: PyYAML in ./.local/lib/python3.10/site-packages (from optuna) (6.0.2)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in ./.local/lib/python3.10/site-packages (from optuna) (2.0.35)\n",
      "Requirement already satisfied: colorlog in ./.local/lib/python3.10/site-packages (from optuna) (6.8.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in ./.local/lib/python3.10/site-packages (from optuna) (1.13.3)\n",
      "Requirement already satisfied: networkx in ./.local/lib/python3.10/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: filelock in ./.local/lib/python3.10/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: fsspec in ./.local/lib/python3.10/site-packages (from torch) (2024.2.0)\n",
      "Requirement already satisfied: jinja2 in ./.local/lib/python3.10/site-packages (from torch) (3.1.3)\n",
      "Requirement already satisfied: typing-extensions in ./.local/lib/python3.10/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in ./.local/lib/python3.10/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in ./.local/lib/python3.10/site-packages (from xgboost) (2.23.4)\n",
      "Requirement already satisfied: plotly in ./.local/lib/python3.10/site-packages (from catboost) (5.24.1)\n",
      "Requirement already satisfied: six in ./.local/lib/python3.10/site-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: graphviz in ./.local/lib/python3.10/site-packages (from catboost) (0.20.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./.local/lib/python3.10/site-packages (from requests) (3.3.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./.local/lib/python3.10/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./.local/lib/python3.10/site-packages (from requests) (3.8)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./.local/lib/python3.10/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: Mako in ./.local/lib/python3.10/site-packages (from alembic>=1.5.0->optuna) (1.3.5)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./.local/lib/python3.10/site-packages (from sqlalchemy>=1.3.0->optuna) (3.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in ./.local/lib/python3.10/site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in ./.local/lib/python3.10/site-packages (from plotly->catboost) (9.0.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in ./.local/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas scikit-learn seaborn matplotlib optuna torch xgboost catboost lightgbm tqdm requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "3fa191c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pprint import pprint\n",
    "import requests\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    make_scorer,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    precision_recall_curve,\n",
    "    recall_score,\n",
    "    silhouette_score,\n",
    ")\n",
    "\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import random\n",
    "import string\n",
    "\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier, plot_metric\n",
    "import lightgbm as lgb\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6a0ca",
   "metadata": {},
   "source": [
    "### 2. Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "51dab8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scorer = make_scorer(f1_score, pos_label=1, average = 'binary')\n",
    "\n",
    "def get_clf_eval(y_test, y_pred=None):\n",
    "    confusion = confusion_matrix(y_test, y_pred, labels=[True, False])\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred, labels=[True, False])\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    F1 = f1_score(y_test, y_pred, labels=[True, False])\n",
    "\n",
    "    print(\"오차행렬:\\n\", confusion)\n",
    "    print(\"\\n정확도: {:.4f}\".format(accuracy))\n",
    "    print(\"정밀도: {:.4f}\".format(precision))\n",
    "    print(\"재현율: {:.4f}\".format(recall))\n",
    "    print(\"F1: {:.4f}\".format(F1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "a6e07c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중요도 시각화\n",
    "def plot_feature_importance(importance_dict, feature_names, title):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, (importance_type, importance) in enumerate(importance_dict.items()):\n",
    "        sorted_idx = np.argsort(importance)[::-1]\n",
    "        sorted_features = np.array(feature_names)[sorted_idx]\n",
    "        sorted_importance = importance[sorted_idx]\n",
    "        \n",
    "        plt.subplot(3, 1, i + 1)\n",
    "        plt.barh(sorted_features, sorted_importance, color='skyblue')\n",
    "        plt.xlabel('Importance')\n",
    "        plt.title(f'Feature Importance by {importance_type.capitalize()}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "944ea95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_weather_data(latitude, longitude, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Open-Meteo API를 사용하여 지정된 기간 동안의 온도와 습도 데이터를 가져옵니다.\n",
    "\n",
    "    Parameters:\n",
    "        latitude (float): 위도\n",
    "        longitude (float): 경도\n",
    "        start_date (str): 시작 날짜 (YYYY-MM-DD)\n",
    "        end_date (str): 종료 날짜 (YYYY-MM-DD)\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: 시간별 온도와 습도가 포함된 DataFrame\n",
    "    \"\"\"\n",
    "    base_url = \"https://archive-api.open-meteo.com/v1/archive\"\n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'start_date': start_date,\n",
    "        'end_date': end_date,\n",
    "        'hourly': 'temperature_2m,relativehumidity_2m',\n",
    "        'timezone': 'Asia/Ho_Chi_Minh'\n",
    "\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(base_url, params=params)\n",
    "        response.raise_for_status()\n",
    "        data = response.json()\n",
    "\n",
    "        # 시간별 데이터 추출\n",
    "        times = data.get('hourly', {}).get('time', [])\n",
    "        temperatures = data.get('hourly', {}).get('temperature_2m', [])\n",
    "        humidities = data.get('hourly', {}).get('relativehumidity_2m', [])\n",
    "\n",
    "        # DataFrame 생성\n",
    "        weather_df = pd.DataFrame({\n",
    "            'DateTime': pd.to_datetime(times),\n",
    "            'Temperature': temperatures,\n",
    "            'Humidity': humidities\n",
    "        })\n",
    "\n",
    "        return weather_df\n",
    "\n",
    "    except requests.exceptions.HTTPError as http_err:\n",
    "        print(f\"HTTP 오류 발생: {http_err}\")\n",
    "    except Exception as err:\n",
    "        print(f\"기타 오류 발생: {err}\")\n",
    "        \n",
    "# 광저우의 위도와 경도\n",
    "LATITUDE = 20.861859116973168\n",
    "LONGITUDE = 106.56603206256099"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd024ec6",
   "metadata": {},
   "source": [
    "## 3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e1d45c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"./data/train.csv\")\n",
    "test = pd.read_csv(\"./data/test.csv\")\n",
    "hand_data = pd.read_excel('data/hand_data.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bc32da",
   "metadata": {},
   "source": [
    "## 4. Features\n",
    "\n",
    "- HEAD NORMAL COORDINATE STAGE1 X좌표의 OK 되어있는 관측치 shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "e8e43c57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10088/4167166692.py:12: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[ 0.     0.012  0.    ...  0.    -0.019  0.   ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  dam.loc[dam['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan]), dam.columns[25:]] = dam_mask\n",
      "/tmp/ipykernel_10088/4167166692.py:26: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[114.612 114.612  85.    ...  85.    114.612  85.   ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  fill2.loc[fill2['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan]), fill2.columns[25:]] = fill2_mask\n",
      "/tmp/ipykernel_10088/4167166692.py:42: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[-0.054  0.     0.    ...  0.     0.     0.   ]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  dam_test.loc[dam_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan]), dam_test.columns[25:]] = dam_mask_test\n",
      "/tmp/ipykernel_10088/4167166692.py:56: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[85. 85. 85. ... 85. 85. 85.]' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  fill2_test.loc[fill2_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan]), fill2_test.columns[25:]] = fill2_mask_test\n"
     ]
    }
   ],
   "source": [
    "# divide\n",
    "dam = train.filter(regex='_Dam')\n",
    "fill1 = train.filter(regex='_Fill1')\n",
    "fill2 = train.filter(regex='_Fill2')\n",
    "autoclave = train.filter(regex='_AutoClave')\n",
    "target = train['target']\n",
    "\n",
    "# dam\n",
    "dam = dam.dropna(axis=1, how='all')\n",
    "dam = dam.drop(columns='HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Dam')\n",
    "dam_mask = dam[dam['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan])].iloc[:, 25:].shift(-1, axis = 1).values\n",
    "dam.loc[dam['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan]), dam.columns[25:]] = dam_mask\n",
    "dam = dam.drop(columns='WorkMode Collect Result_Dam')\n",
    "\n",
    "# fill1\n",
    "fill1 = fill1.dropna(axis=1, how='all')\n",
    "fill1 = fill1.drop(columns='HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill1')\n",
    "fill1_mask = fill1[fill1['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].isin(['OK', np.nan])].iloc[:, 15:].shift(-1, axis = 1).values\n",
    "fill1.loc[fill1['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].isin(['OK', np.nan]), fill1.columns[15:]] = fill1_mask\n",
    "fill1 = fill1.drop(columns='WorkMode Collect Result_Fill1')\n",
    "\n",
    "# fill2\n",
    "fill2 = fill2.dropna(axis=1, how='all')\n",
    "fill2 = fill2.drop(columns='HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill2')\n",
    "fill2_mask = fill2[fill2['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan])].iloc[:, 25:].shift(-1, axis = 1).values\n",
    "fill2.loc[fill2['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan]), fill2.columns[25:]] = fill2_mask\n",
    "fill2 = fill2.drop(columns='WorkMode Collect Result_Fill2')\n",
    "\n",
    "# CONCAT\n",
    "train = pd.concat([dam, fill1, fill2, autoclave, target], axis=1)\n",
    "\n",
    "# divide\n",
    "dam_test = test.filter(regex='_Dam')\n",
    "fill1_test = test.filter(regex='_Fill1')\n",
    "fill2_test = test.filter(regex='_Fill2')\n",
    "autoclave_test = test.filter(regex='_AutoClave')\n",
    "\n",
    "# dam\n",
    "dam_test = dam_test.dropna(axis=1, how='all')\n",
    "dam_test = dam_test.drop(columns='HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Dam')\n",
    "dam_mask_test = dam_test[dam_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan])].iloc[:, 25:].shift(-1, axis = 1).values\n",
    "dam_test.loc[dam_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].isin(['OK', np.nan]), dam_test.columns[25:]] = dam_mask_test\n",
    "dam_test = dam_test.drop(columns='WorkMode Collect Result_Dam')\n",
    "\n",
    "# fill1\n",
    "fill1_test = fill1_test.dropna(axis=1, how='all')\n",
    "fill1_test = fill1_test.drop(columns='HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill1')\n",
    "fill1_mask_test = fill1_test[fill1_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].isin(['OK', np.nan])].iloc[:, 15:].shift(-1, axis = 1).values\n",
    "fill1_test.loc[fill1_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].isin(['OK', np.nan]), fill1_test.columns[15:]] = fill1_mask_test\n",
    "fill1_test = fill1_test.drop(columns='WorkMode Collect Result_Fill1')\n",
    "\n",
    "# fill2\n",
    "fill2_test = fill2_test.dropna(axis=1, how='all')\n",
    "fill2_test = fill2_test.drop(columns='HEAD NORMAL COORDINATE X AXIS(Stage1) Judge Value_Fill2')\n",
    "fill2_mask_test = fill2_test[fill2_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan])].iloc[:, 25:].shift(-1, axis = 1).values\n",
    "fill2_test.loc[fill2_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2'].isin(['OK', np.nan]), fill2_test.columns[25:]] = fill2_mask_test\n",
    "fill2_test = fill2_test.drop(columns='WorkMode Collect Result_Fill2')\n",
    "\n",
    "# CONCAT\n",
    "test = pd.concat([dam_test, fill1_test, fill2_test, autoclave_test, test['target']], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803a3f6a",
   "metadata": {},
   "source": [
    "- Dam, Fill1에 대해 stage별 잘못 기입되어 있는 값 swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "7a0effab",
   "metadata": {},
   "outputs": [],
   "source": [
    "train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] = train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].astype(float)\n",
    "train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'] = train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'].astype(float)\n",
    "train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'] = train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'].astype(float)\n",
    "\n",
    "test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] = test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].astype(float)\n",
    "test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'] = test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'].astype(float)\n",
    "test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'] = test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'].astype(float)\n",
    "\n",
    "# 조건 Dam dispenser #1: Equipment_Dam == 'Dam dispenser #1' and Stage1 < 500\n",
    "condition1 = (train['Equipment_Dam'] == 'Dam dispenser #1') & (train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] < 500)\n",
    "train.loc[condition1, 'Equipment_Dam'] = 'Dam dispenser #2'\n",
    "\n",
    "# 조건 Dam dispenser #2: Equipment_Dam == 'Dam dispenser #1' and Stage2 > 800\n",
    "condition2 = (train['Equipment_Dam'] == 'Dam dispenser #1') & (train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'].astype(float) > 800)\n",
    "train.loc[condition2, 'Equipment_Dam'] = 'Dam dispenser #2'\n",
    "\n",
    "# 조건 3: Equipment_Dam == 'Dam dispenser #1' and Stage3 > 800\n",
    "condition3 = (train['Equipment_Dam'] == 'Dam dispenser #1') & (train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'] > 800)\n",
    "train.loc[condition3, 'Equipment_Dam'] = 'Dam dispenser #2'\n",
    "\n",
    "# 조건 4: Equipment_Dam == 'Dam dispenser #2' and Stage1 < 500\n",
    "condition4 = (train['Equipment_Dam'] == 'Dam dispenser #2') & (train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] < 500)\n",
    "train.loc[condition4, 'Equipment_Dam'] = 'Dam dispenser #1'\n",
    "\n",
    "# 조건 5: Equipment_Dam == 'Dam dispenser #2' and Stage2 > 500\n",
    "condition5 = (train['Equipment_Dam'] == 'Dam dispenser #2') & (train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'] > 500)\n",
    "train.loc[condition5, 'Equipment_Dam'] = 'Dam dispenser #1'\n",
    "\n",
    "# 조건 6: Equipment_Dam == 'Dam dispenser #2' and Stage3 > 800\n",
    "condition6 = (train['Equipment_Dam'] == 'Dam dispenser #2') & (train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'] > 800)\n",
    "train.loc[condition6, 'Equipment_Dam'] = 'Dam dispenser #1'\n",
    "\n",
    "\n",
    "\n",
    "# 조건 Dam dispenser #1: Equipment_Dam == 'Dam dispenser #1' and Stage1 < 500\n",
    "condition1 = (test['Equipment_Dam'] == 'Dam dispenser #1') & (test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] < 500)\n",
    "test.loc[condition1, 'Equipment_Dam'] = 'Dam dispenser #2'\n",
    "\n",
    "# 조건 Dam dispenser #2: Equipment_Dam == 'Dam dispenser #1' and Stage2 > 800\n",
    "condition2 = (test['Equipment_Dam'] == 'Dam dispenser #1') & (test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'].astype(float) > 800)\n",
    "test.loc[condition2, 'Equipment_Dam'] = 'Dam dispenser #2'\n",
    "\n",
    "# 조건 3: Equipment_Dam == 'Dam dispenser #1' and Stage3 > 800\n",
    "condition3 = (test['Equipment_Dam'] == 'Dam dispenser #1') & (test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'] > 800)\n",
    "test.loc[condition3, 'Equipment_Dam'] = 'Dam dispenser #2'\n",
    "\n",
    "# 조건 4: Equipment_Dam == 'Dam dispenser #2' and Stage1 < 500\n",
    "condition4 = (test['Equipment_Dam'] == 'Dam dispenser #2') & (test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] < 500)\n",
    "test.loc[condition4, 'Equipment_Dam'] = 'Dam dispenser #1'\n",
    "\n",
    "# 조건 5: Equipment_Dam == 'Dam dispenser #2' and Stage2 > 500\n",
    "condition5 = (test['Equipment_Dam'] == 'Dam dispenser #2') & (test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'] > 500)\n",
    "test.loc[condition5, 'Equipment_Dam'] = 'Dam dispenser #1'\n",
    "\n",
    "# 조건 6: Equipment_Dam == 'Dam dispenser #2' and Stage3 > 800\n",
    "condition6 = (test['Equipment_Dam'] == 'Dam dispenser #2') & (test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'] > 800)\n",
    "test.loc[condition6, 'Equipment_Dam'] = 'Dam dispenser #1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "871bf599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] = train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float)\n",
    "train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] = train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float)\n",
    "\n",
    "\n",
    "# 이동 전\n",
    "X_sum_down_1 = train[train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float) < 500]['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float).mean()\n",
    "X_sum_down_2 = train[train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) < 500]['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float).mean()\n",
    "X_sum_up_1 = train[train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float) > 500]['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float).mean()\n",
    "X_sum_up_2 = train[train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) > 500]['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float).mean()\n",
    "\n",
    "X_sum_down = (X_sum_down_1 - X_sum_down_2) / 2 # stage1에서 빼고, Stage3에서 더하기 <500\n",
    "X_sum_up = (X_sum_up_2 - X_sum_up_1) / 2 # stage1에서 더하고, Stage 3에서 빼기\n",
    "\n",
    "train.loc[train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] += X_sum_up\n",
    "train.loc[train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] -= X_sum_up\n",
    "\n",
    "train.loc[train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float) < 500, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] -= X_sum_down\n",
    "train.loc[train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) < 500, 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] += X_sum_down\n",
    "\n",
    "# test\n",
    "test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] = test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float)\n",
    "test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] = test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float)\n",
    "\n",
    "\n",
    "# 이동 전\n",
    "test.loc[test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] += X_sum_up\n",
    "test.loc[test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] -= X_sum_up\n",
    "test.loc[test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float) < 500, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] -= X_sum_down\n",
    "test.loc[test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) < 500, 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] += X_sum_down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "eeb75fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "Y_sum_dam_1 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float) < 500]['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float).mean()\n",
    "Y_sum_dam_2 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float) > 500]['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float).mean()\n",
    "\n",
    "train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] = Y_sum_dam_1 + Y_sum_dam_2 - train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam']\n",
    "\n",
    "Y_sum_dam_3 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float) < 500]['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float).mean()\n",
    "Y_sum_dam_4 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float) > 500]['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float).mean()\n",
    "\n",
    "train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'] = Y_sum_dam_3 + Y_sum_dam_4 - train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "Y_sum_dam_5 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float) < 500]['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float).mean()\n",
    "Y_sum_dam_6 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float) > 500]['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float).mean()\n",
    "\n",
    "train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] = Y_sum_dam_5 + Y_sum_dam_6 - train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam']\n",
    "\n",
    "Y_sum_fill_1 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float) > 500]['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float).mean()\n",
    "Y_sum_fill_2 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float) < 500]['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float).mean()\n",
    "\n",
    "train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] = Y_sum_fill_1 + Y_sum_fill_2 - train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1']\n",
    "\n",
    "Y_sum_fill_3 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float) > 500]['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float).mean()\n",
    "Y_sum_fill_4 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float) < 500]['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float).mean()\n",
    "\n",
    "train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'] = Y_sum_fill_3 + Y_sum_fill_4 - train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1']\n",
    "\n",
    "Y_sum_fill_5 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500]['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float).mean()\n",
    "Y_sum_fill_6 = train[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float) < 500]['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float).mean()\n",
    "train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'] = Y_sum_fill_5 + Y_sum_fill_6 - train.loc[train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1']\n",
    "\n",
    "\n",
    "# test\n",
    "test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] = Y_sum_dam_1 + Y_sum_dam_2 - test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam']\n",
    "test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] = Y_sum_dam_5 + Y_sum_dam_6 - test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam']\n",
    "test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'] = Y_sum_dam_3 + Y_sum_dam_4 - test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] = Y_sum_fill_1 + Y_sum_fill_2 - test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1']\n",
    "test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'] = Y_sum_fill_5 + Y_sum_fill_6 - test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1']\n",
    "test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'] = Y_sum_fill_3 + Y_sum_fill_4 - test.loc[test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].astype(float) > 500, 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "b180619f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_columns(df, condition, col1, col2):\n",
    "    # 조건에 해당하는 행 필터링\n",
    "    filtered_df = df[condition]\n",
    "    \n",
    "    # 값 교환\n",
    "    df.loc[condition, [col1, col2]] = filtered_df[[col1, col2]].copy().iloc[:, ::-1].values\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "72a44ab1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 조건을 만족하는 행 인덱스를 찾음\n",
    "condition = train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) >= 200\n",
    "\n",
    "# DISCHARGED TIME OF RESIN(Stage1) \n",
    "swap_columns(train, condition, 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam', 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam')\n",
    "\n",
    "# Dispense Volume(Stage1)\n",
    "swap_columns(train, condition, 'Dispense Volume(Stage1) Collect Result_Dam', 'Dispense Volume(Stage3) Collect Result_Dam')\n",
    "\n",
    "# HEAD NORMAL COORDINATE Y AXIS(Stage1)\n",
    "swap_columns(train, condition, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam')\n",
    "\n",
    "# HEAD NORMAL COORDINATE Z AXIS(Stage1)\n",
    "swap_columns(train, condition, 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam')\n",
    "\n",
    "# Stage1 Circle1 Distance Speed Collect\n",
    "swap_columns(train, condition, 'Stage1 Circle1 Distance Speed Collect Result_Dam', 'Stage3 Circle1 Distance Speed Collect Result_Dam')\n",
    "swap_columns(train, condition, 'Stage1 Circle2 Distance Speed Collect Result_Dam', 'Stage3 Circle2 Distance Speed Collect Result_Dam')\n",
    "swap_columns(train, condition, 'Stage1 Circle3 Distance Speed Collect Result_Dam', 'Stage3 Circle3 Distance Speed Collect Result_Dam')\n",
    "swap_columns(train, condition, 'Stage1 Circle4 Distance Speed Collect Result_Dam', 'Stage3 Circle4 Distance Speed Collect Result_Dam')\n",
    "\n",
    "# Stage1 Line1 Distance Speed Collect\n",
    "swap_columns(train, condition, 'Stage1 Line1 Distance Speed Collect Result_Dam', 'Stage3 Line1 Distance Speed Collect Result_Dam')\n",
    "swap_columns(train, condition, 'Stage1 Line2 Distance Speed Collect Result_Dam', 'Stage3 Line2 Distance Speed Collect Result_Dam')\n",
    "swap_columns(train, condition, 'Stage1 Line3 Distance Speed Collect Result_Dam', 'Stage3 Line3 Distance Speed Collect Result_Dam')\n",
    "swap_columns(train, condition, 'Stage1 Line4 Distance Speed Collect Result_Dam', 'Stage3 Line4 Distance Speed Collect Result_Dam')\n",
    "\n",
    "# THICKNESS 1\n",
    "swap_columns(train, condition, 'THICKNESS 1 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam')\n",
    "\n",
    "### 젤 마지막에 와야됨!!!!\n",
    "# HEAD NORMAL COORDINATE X AXIS(Stage1)\n",
    "swap_columns(train, condition, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam')\n",
    "\n",
    "\n",
    "### Test\n",
    "# 조건을 만족하는 행 인덱스를 찾음\n",
    "condition = test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float) >= 200\n",
    "\n",
    "# DISCHARGED TIME OF RESIN(Stage1) \n",
    "swap_columns(test, condition, 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam', 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam')\n",
    "\n",
    "# Dispense Volume(Stage1)\n",
    "swap_columns(test, condition, 'Dispense Volume(Stage1) Collect Result_Dam', 'Dispense Volume(Stage3) Collect Result_Dam')\n",
    "\n",
    "# HEAD NORMAL COORDINATE Y AXIS(Stage1)\n",
    "swap_columns(test, condition, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam')\n",
    "\n",
    "# HEAD NORMAL COORDINATE Z AXIS(Stage1)\n",
    "swap_columns(test, condition, 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam')\n",
    "\n",
    "# Stage1 Circle1 Distance Speed Collect\n",
    "swap_columns(test, condition, 'Stage1 Circle1 Distance Speed Collect Result_Dam', 'Stage3 Circle1 Distance Speed Collect Result_Dam')\n",
    "swap_columns(test, condition, 'Stage1 Circle2 Distance Speed Collect Result_Dam', 'Stage3 Circle2 Distance Speed Collect Result_Dam')\n",
    "swap_columns(test, condition, 'Stage1 Circle3 Distance Speed Collect Result_Dam', 'Stage3 Circle3 Distance Speed Collect Result_Dam')\n",
    "swap_columns(test, condition, 'Stage1 Circle4 Distance Speed Collect Result_Dam', 'Stage3 Circle4 Distance Speed Collect Result_Dam')\n",
    "\n",
    "# Stage1 Line1 Distance Speed Collect\n",
    "swap_columns(test, condition, 'Stage1 Line1 Distance Speed Collect Result_Dam', 'Stage3 Line1 Distance Speed Collect Result_Dam')\n",
    "swap_columns(test, condition, 'Stage1 Line2 Distance Speed Collect Result_Dam', 'Stage3 Line2 Distance Speed Collect Result_Dam')\n",
    "swap_columns(test, condition, 'Stage1 Line3 Distance Speed Collect Result_Dam', 'Stage3 Line3 Distance Speed Collect Result_Dam')\n",
    "swap_columns(test, condition, 'Stage1 Line4 Distance Speed Collect Result_Dam', 'Stage3 Line4 Distance Speed Collect Result_Dam')\n",
    "\n",
    "# THICKNESS 1\n",
    "swap_columns(test, condition, 'THICKNESS 1 Collect Result_Dam', 'THICKNESS 3 Collect Result_Dam')\n",
    "\n",
    "### 젤 마지막에 와야됨!!!!\n",
    "# HEAD NORMAL COORDINATE X AXIS(Stage1)\n",
    "swap_columns(test, condition, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "ac95e606",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Train\n",
    "condition = train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500\n",
    "\n",
    "# DISCHARGED TIME OF RESIN(Stage1)\n",
    "swap_columns(train, condition, 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1', 'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1')\n",
    "\n",
    "# Dispense Volume(Stage1)\n",
    "swap_columns(train, condition, 'Dispense Volume(Stage1) Collect Result_Fill1', 'Dispense Volume(Stage2) Collect Result_Fill1')\n",
    "\n",
    "# HEAD NORMAL COORDINATE Y AXIS(Stage1)\n",
    "swap_columns(train, condition, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1')\n",
    "\n",
    "# HEAD NORMAL COORDINATE Z AXIS(Stage1)\n",
    "swap_columns(train, condition, 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1')\n",
    "\n",
    "# 반드시 마지막으로 와야함!!!!\n",
    "# HEAD NORMAL COORDINATE X AXIS(Stage1)\n",
    "swap_columns(train, condition, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1')\n",
    "\n",
    "### Test\n",
    "condition = test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'].astype(float) > 500\n",
    "\n",
    "# DISCHARGED TIME OF RESIN(Stage1)\n",
    "swap_columns(test, condition, 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1', 'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1')\n",
    "\n",
    "# Dispense Volume(Stage1)\n",
    "swap_columns(test, condition, 'Dispense Volume(Stage1) Collect Result_Fill1', 'Dispense Volume(Stage2) Collect Result_Fill1')\n",
    "\n",
    "# HEAD NORMAL COORDINATE Y AXIS(Stage1)\n",
    "swap_columns(test, condition, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1')\n",
    "\n",
    "# HEAD NORMAL COORDINATE Z AXIS(Stage1)\n",
    "swap_columns(test, condition, 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1')\n",
    "\n",
    "# 반드시 마지막으로 와야함!!!!\n",
    "# HEAD NORMAL COORDINATE X AXIS(Stage1)\n",
    "swap_columns(test, condition, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "cc8ca5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### Train\n",
    "# 조건을 만족하는 행 인덱스를 찾음\n",
    "condition = train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].astype(float) < 200\n",
    "\n",
    "# DISCHARGED TIME OF RESIN(Stage1)\n",
    "swap_columns(train, condition, 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1', 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1')\n",
    "\n",
    "# Dispense Volume(Stage1)\n",
    "swap_columns(train, condition, 'Dispense Volume(Stage1) Collect Result_Fill1', 'Dispense Volume(Stage3) Collect Result_Fill1')\n",
    "\n",
    "# HEAD NORMAL COORDINATE Y AXIS(Stage1)\n",
    "swap_columns(train, condition, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1')\n",
    "\n",
    "# HEAD NORMAL COORDINATE Z AXIS(Stage1)\n",
    "swap_columns(train, condition, 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1')\n",
    "\n",
    "# 반드시 마지막으로 와야함!!!!\n",
    "# HEAD NORMAL COORDINATE X AXIS(Stage1)\n",
    "swap_columns(train, condition, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1')\n",
    "\n",
    "### Test\n",
    "condition = test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].astype(float) < 200\n",
    "\n",
    "# DISCHARGED TIME OF RESIN(Stage1)\n",
    "swap_columns(test, condition, 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1', 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1')\n",
    "\n",
    "# Dispense Volume(Stage1)\n",
    "swap_columns(test, condition, 'Dispense Volume(Stage1) Collect Result_Fill1', 'Dispense Volume(Stage3) Collect Result_Fill1')\n",
    "\n",
    "# HEAD NORMAL COORDINATE Y AXIS(Stage1)\n",
    "swap_columns(test, condition, 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1')\n",
    "\n",
    "# HEAD NORMAL COORDINATE Z AXIS(Stage1)\n",
    "swap_columns(test, condition, 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1')\n",
    "\n",
    "# 반드시 마지막으로 와야함!!!!\n",
    "# HEAD NORMAL COORDINATE X AXIS(Stage1)\n",
    "swap_columns(test, condition, 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1', 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "eaaa2cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = train\n",
    "df_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "ed4fec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Object형 데이터 float로 바꿔주기\n",
    "type_change = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam', 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam', 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1']\n",
    "\n",
    "for i in type_change:\n",
    "    df_train[i] = df_train[i].astype(float)\n",
    "    df_test[i] = df_test[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "id": "b6cfb97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equipment 숫자 1, 2로만 표시하기\n",
    "train = df_train\n",
    "test = df_test\n",
    "train['Equipment_Dam'] = train['Equipment_Dam'].str.slice(15, 16)\n",
    "train['Equipment_Fill1'] = train['Equipment_Fill1'].str.slice(17, 18)\n",
    "train['Equipment_Fill2'] = train['Equipment_Fill2'].str.slice(17, 18)\n",
    "\n",
    "test['Equipment_Dam'] = test['Equipment_Dam'].str.slice(15, 16)\n",
    "test['Equipment_Fill1'] = test['Equipment_Fill1'].str.slice(17, 18)\n",
    "test['Equipment_Fill2'] = test['Equipment_Fill2'].str.slice(17, 18)\n",
    "df_train = train\n",
    "df_test = test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "c6f5c98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dam, Fill1, Fill2에서 지정된 값이 다를 경우 Abnormal \n",
    "def inconsistant(data, columnname, iwantthiscolumnsname, is_train = True):\n",
    "    # 장비 번호가 다르면 불일치\n",
    "    if is_train:\n",
    "        cri = [\n",
    "            df_train[columnname + '_Dam'] != df_train[columnname + '_Fill1'],\n",
    "            df_train[columnname + '_Dam'] != df_train[columnname + '_Fill2'],\n",
    "            df_train[columnname + '_Fill1'] != df_train[columnname + '_Fill2'],\n",
    "            data[iwantthiscolumnsname] == 1\n",
    "        ]\n",
    "        \n",
    "    else:\n",
    "        cri = [\n",
    "            df_test[columnname + '_Dam'] != df_test[columnname + '_Fill1'],\n",
    "            df_test[columnname + '_Dam'] != df_test[columnname + '_Fill2'],\n",
    "            df_test[columnname + '_Fill1'] != df_test[columnname + '_Fill1'],\n",
    "            data[iwantthiscolumnsname] == 1\n",
    "        ]\n",
    "    con = [1, 1, 1, 1]\n",
    "\n",
    "    data[iwantthiscolumnsname] = np.select(cri, con, default = 0)\n",
    "    \n",
    "# 불일치 변수\n",
    "df_train['inconsistant'] = 0\n",
    "df_test['inconsistant'] = 0\n",
    "\n",
    "# 기준\n",
    "columnname = ['Equipment', 'Receip No Collect Result', 'Production Qty Collect Result', 'PalletID Collect Result', ]\n",
    "\n",
    "# 장착\n",
    "for i in columnname:\n",
    "    inconsistant(df_train, i, 'inconsistant', True)\n",
    "    inconsistant(df_test, i, 'inconsistant', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "id": "c15840cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시간이 0이하, 900이상인 값은 이상치로 분류\n",
    "for j in ['Machine Tact time Collect Result_Dam', 'Machine Tact time Collect Result_Fill1', 'Machine Tact time Collect Result_Fill2']:\n",
    "    cri = [\n",
    "        df_train[j] <= 0,\n",
    "        df_train[j] > 900\n",
    "    ]\n",
    "    cri2 = [\n",
    "        df_test[j] <= 0,\n",
    "        df_test[j] > 900\n",
    "    ]\n",
    "    con = [\n",
    "        1, 1\n",
    "    ]\n",
    "    df_train['inconsistant'] = np.select(cri, con, default = df_train['inconsistant'])\n",
    "    df_test['inconsistant'] = np.select(cri2, con, default = df_test['inconsistant'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "id": "ed536bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Equipment에 대한 단일화 작업\n",
    "df_train['Equipment'] = df_train.apply(\n",
    "    lambda row: row['Equipment_Dam']\n",
    "    if row['Equipment_Dam'] == row['Equipment_Fill1'] == row['Equipment_Fill2']\n",
    "    else -1, axis=1\n",
    ")\n",
    "\n",
    "df_train = df_train.drop(columns=['Equipment_Dam', 'Equipment_Fill1', 'Equipment_Fill2'])\n",
    "\n",
    "df_test['Equipment'] = df_test.apply(\n",
    "    lambda row: row['Equipment_Dam']\n",
    "    if row['Equipment_Dam'] == row['Equipment_Fill1'] == row['Equipment_Fill2']\n",
    "    else -1, axis=1\n",
    ")\n",
    "\n",
    "df_test = df_test.drop(columns=['Equipment_Dam', 'Equipment_Fill1', 'Equipment_Fill2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "id": "29dac21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# QTY의 단일화\n",
    "df_train['Production Qty'] = df_train.apply(\n",
    "    lambda row: row['Production Qty Collect Result_Dam']\n",
    "    if row['Production Qty Collect Result_Dam'] == row['Production Qty Collect Result_Fill1'] == row['Production Qty Collect Result_Fill2']\n",
    "    else -1, axis=1\n",
    ")\n",
    "\n",
    "df_train = df_train.drop(columns = ['Production Qty Collect Result_Dam', 'Production Qty Collect Result_Fill1', 'Production Qty Collect Result_Fill2'])\n",
    "\n",
    "df_test['Production Qty'] = df_test.apply(\n",
    "    lambda row: row['Production Qty Collect Result_Dam']\n",
    "    if row['Production Qty Collect Result_Dam'] == row['Production Qty Collect Result_Fill1'] == row['Production Qty Collect Result_Fill2']\n",
    "    else -1, axis=1\n",
    ")\n",
    "\n",
    "df_test = df_test.drop(columns = ['Production Qty Collect Result_Dam', 'Production Qty Collect Result_Fill1', 'Production Qty Collect Result_Fill2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "id": "247e31e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Receip No에 대한 단일화 작업\n",
    "df_train['Receip No'] = df_train.apply(\n",
    "    lambda row: row['Receip No Collect Result_Dam']\n",
    "    if row['Receip No Collect Result_Dam'] == row['Receip No Collect Result_Fill1'] == row['Receip No Collect Result_Fill2']\n",
    "    else -1, axis=1\n",
    ")\n",
    "\n",
    "df_train = df_train.drop(columns=['Receip No Collect Result_Dam', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2'])\n",
    "\n",
    "df_test['Receip No'] = df_test.apply(\n",
    "    lambda row: row['Receip No Collect Result_Dam']\n",
    "    if row['Receip No Collect Result_Dam'] == row['Receip No Collect Result_Fill1'] == row['Receip No Collect Result_Fill2']\n",
    "    else -1, axis=1\n",
    ")\n",
    "\n",
    "df_test = df_test.drop(columns=['Receip No Collect Result_Dam', 'Receip No Collect Result_Fill1', 'Receip No Collect Result_Fill2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "7d2dcc15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PalletID에 대한 단일화 작업\n",
    "df_train['PalletID'] = df_train.apply(\n",
    "    lambda row: row['PalletID Collect Result_Dam']\n",
    "    if row['PalletID Collect Result_Dam'] == row['PalletID Collect Result_Fill1'] == row['PalletID Collect Result_Fill2']\n",
    "    else -1, axis=1\n",
    ")\n",
    "\n",
    "df_train = df_train.drop(columns=['PalletID Collect Result_Dam', 'PalletID Collect Result_Fill1', 'PalletID Collect Result_Fill2'])\n",
    "\n",
    "df_test['PalletID'] = df_test.apply(\n",
    "    lambda row: row['PalletID Collect Result_Dam']\n",
    "    if row['PalletID Collect Result_Dam'] == row['PalletID Collect Result_Fill1'] == row['PalletID Collect Result_Fill2']\n",
    "    else -1, axis=1\n",
    ")\n",
    "\n",
    "df_test = df_test.drop(columns=['PalletID Collect Result_Dam', 'PalletID Collect Result_Fill1', 'PalletID Collect Result_Fill2'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "87bd262a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 좌표 차이 구하기\n",
    "df_train['Minus1_Dam']= df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam']\n",
    "df_train['Minus2_Dam']= df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_test['Minus1_Dam']= df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam']\n",
    "df_test['Minus2_Dam']= df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_train['Minus1Y_Dam']= df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam']\n",
    "df_train['Minus2Y_Dam']= df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_test['Minus1Y_Dam']= df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam']\n",
    "df_test['Minus2Y_Dam']= df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam']\n",
    "\n",
    "df_train['Minus1Y_Dam'] = df_train['Minus1Y_Dam'].apply(lambda x: 1 if x > 2 or x < -2 else 0)\n",
    "df_train['Minus2Y_Dam'] = df_train['Minus2Y_Dam'].apply(lambda x: 1 if x > 2 or x < -2 else 0)\n",
    "\n",
    "df_test['Minus1Y_Dam'] = df_test['Minus1Y_Dam'].apply(lambda x: 1 if x > 2 or x < -2 else 0)\n",
    "df_test['Minus2Y_Dam'] = df_test['Minus2Y_Dam'].apply(lambda x: 1 if x > 2 or x < -2 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "id": "272189cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## 좌표에 대해 표준편차 구하기\n",
    "\n",
    "# Fill1 \n",
    "# Train\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) E Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].astype(float)\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) E Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].mean()\n",
    "df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) E Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].mean()\n",
    "df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) E Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'].mean()\n",
    "\n",
    "df_train['HEAD NORMAL COORDINATE Error (Stage1)_Fill1'] = np.sqrt(df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) E Collect Result_Fill1'] ** 2 + \\\n",
    "                                                               df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) E Collect Result_Fill1'] ** 2 + \\\n",
    "                                                               df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) E Collect Result_Fill1'] ** 2)\n",
    "\n",
    "\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) E Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'].astype(float)\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) E Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'].mean()\n",
    "df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) E Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].mean()\n",
    "df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) E Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1'].mean()\n",
    "\n",
    "df_train['HEAD NORMAL COORDINATE Error (Stage2)_Fill1'] = np.sqrt(df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) E Collect Result_Fill1'] ** 2 + \\\n",
    "                                                               df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) E Collect Result_Fill1'] ** 2 + \\\n",
    "                                                               df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) E Collect Result_Fill1'] ** 2)\n",
    "\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) E Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'].astype(float)\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) E Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'].mean()\n",
    "df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) E Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].mean()\n",
    "df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) E Collect Result_Fill1'] = df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1'] - df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1'].mean()\n",
    "\n",
    "df_train['HEAD NORMAL COORDINATE Error (Stage3)_Fill1'] = np.sqrt(df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) E Collect Result_Fill1'] ** 2 + \\\n",
    "                                                               df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) E Collect Result_Fill1'] ** 2 + \\\n",
    "                                                               df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) E Collect Result_Fill1'] ** 2)\n",
    "\n",
    "# Test\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) E Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].astype(float)\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) E Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1'].mean()\n",
    "df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) E Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1'].mean()\n",
    "df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) E Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1'].mean()\n",
    "\n",
    "df_test['HEAD NORMAL COORDINATE Error (Stage1)_Fill1'] = np.sqrt(df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) E Collect Result_Fill1'] ** 2 + \\\n",
    "                                                               df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) E Collect Result_Fill1'] ** 2 + \\\n",
    "                                                               df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) E Collect Result_Fill1'] ** 2)\n",
    "\n",
    "\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) E Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'].astype(float)\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) E Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1'].mean()\n",
    "df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) E Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1'].mean()\n",
    "df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) E Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1'].mean()\n",
    "\n",
    "df_test['HEAD NORMAL COORDINATE Error (Stage2)_Fill1'] = np.sqrt(df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) E Collect Result_Fill1'] ** 2 + \\\n",
    "                                                               df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) E Collect Result_Fill1'] ** 2 + \\\n",
    "                                                               df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) E Collect Result_Fill1'] ** 2)\n",
    "\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) E Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'].astype(float)\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) E Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1'].mean()\n",
    "df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) E Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1'].mean()\n",
    "df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) E Collect Result_Fill1'] = df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1'] - df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1'].mean()\n",
    "\n",
    "df_test['HEAD NORMAL COORDINATE Error (Stage3)_Fill1'] = np.sqrt(df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) E Collect Result_Fill1'] ** 2 + \\\n",
    "                                                               df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) E Collect Result_Fill1'] ** 2 + \\\n",
    "                                                               df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) E Collect Result_Fill1'] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "id": "652bd1d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dam\n",
    "# Train\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) E Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float)\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) E Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].mean()\n",
    "df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) E Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].mean()\n",
    "df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) E Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam'].mean()\n",
    "\n",
    "df_train['HEAD NORMAL COORDINATE Error (Stage1)_Dam'] = np.sqrt(df_train['HEAD NORMAL COORDINATE X AXIS(Stage1) E Collect Result_Dam'] ** 2 + \\\n",
    "                                                               df_train['HEAD NORMAL COORDINATE Y AXIS(Stage1) E Collect Result_Dam'] ** 2 + \\\n",
    "                                                               df_train['HEAD NORMAL COORDINATE Z AXIS(Stage1) E Collect Result_Dam'] ** 2)\n",
    "\n",
    "\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) E Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'].astype(float)\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) E Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'].mean()\n",
    "df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) E Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].mean()\n",
    "df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) E Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam'].mean()\n",
    "\n",
    "df_train['HEAD NORMAL COORDINATE Error (Stage2)_Dam'] = np.sqrt(df_train['HEAD NORMAL COORDINATE X AXIS(Stage2) E Collect Result_Dam'] ** 2 + \\\n",
    "                                                               df_train['HEAD NORMAL COORDINATE Y AXIS(Stage2) E Collect Result_Dam'] ** 2 + \\\n",
    "                                                               df_train['HEAD NORMAL COORDINATE Z AXIS(Stage2) E Collect Result_Dam'] ** 2)\n",
    "\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) E Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float)\n",
    "df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) E Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].mean()\n",
    "df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) E Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].mean()\n",
    "df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) E Collect Result_Dam'] = df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam'] - df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam'].mean()\n",
    "\n",
    "df_train['HEAD NORMAL COORDINATE Error (Stage3)_Dam'] = np.sqrt(df_train['HEAD NORMAL COORDINATE X AXIS(Stage3) E Collect Result_Dam'] ** 2 + \\\n",
    "                                                               df_train['HEAD NORMAL COORDINATE Y AXIS(Stage3) E Collect Result_Dam'] ** 2 + \\\n",
    "                                                               df_train['HEAD NORMAL COORDINATE Z AXIS(Stage3) E Collect Result_Dam'] ** 2)\n",
    "\n",
    "# Test\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) E Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].astype(float)\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) E Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam'].mean()\n",
    "df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) E Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam'].mean()\n",
    "df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) E Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam'].mean()\n",
    "\n",
    "df_test['HEAD NORMAL COORDINATE Error (Stage1)_Dam'] = np.sqrt(df_test['HEAD NORMAL COORDINATE X AXIS(Stage1) E Collect Result_Dam'] ** 2 + \\\n",
    "                                                               df_test['HEAD NORMAL COORDINATE Y AXIS(Stage1) E Collect Result_Dam'] ** 2 + \\\n",
    "                                                               df_test['HEAD NORMAL COORDINATE Z AXIS(Stage1) E Collect Result_Dam'] ** 2)\n",
    "\n",
    "\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) E Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'].astype(float)\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) E Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam'].mean()\n",
    "df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) E Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam'].mean()\n",
    "df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) E Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam'].mean()\n",
    "\n",
    "df_test['HEAD NORMAL COORDINATE Error (Stage2)_Dam'] = np.sqrt(df_test['HEAD NORMAL COORDINATE X AXIS(Stage2) E Collect Result_Dam'] ** 2 + \\\n",
    "                                                               df_test['HEAD NORMAL COORDINATE Y AXIS(Stage2) E Collect Result_Dam'] ** 2 + \\\n",
    "                                                               df_test['HEAD NORMAL COORDINATE Z AXIS(Stage2) E Collect Result_Dam'] ** 2)\n",
    "\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) E Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].astype(float)\n",
    "df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) E Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam'].mean()\n",
    "df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) E Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam'].mean()\n",
    "df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) E Collect Result_Dam'] = df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam'] - df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam'].mean()\n",
    "\n",
    "df_test['HEAD NORMAL COORDINATE Error (Stage3)_Dam'] = np.sqrt(df_test['HEAD NORMAL COORDINATE X AXIS(Stage3) E Collect Result_Dam'] ** 2 + \\\n",
    "                                                               df_test['HEAD NORMAL COORDINATE Y AXIS(Stage3) E Collect Result_Dam'] ** 2 + \\\n",
    "                                                               df_test['HEAD NORMAL COORDINATE Z AXIS(Stage3) E Collect Result_Dam'] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "id": "3eda61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 압력값 보정 및 온도와 곱하기\n",
    "df_train['1st Pressure Power_AutoClave'] = (1 - df_train['1st Pressure Collect Result_AutoClave']) * df_train['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "df_train['2nd Pressure Power_AutoClave'] = (1 - df_train['2nd Pressure Collect Result_AutoClave']) * df_train['2nd Pressure Unit Time_AutoClave']\n",
    "df_train['3rd Pressure Power_AutoClave'] = (1 - df_train['3rd Pressure Collect Result_AutoClave']) * df_train['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "df_train['1st Power x Temp_AutoCLave'] = df_train['1st Pressure Power_AutoClave'] * df_train['Chamber Temp. Collect Result_AutoClave']\n",
    "df_train['2nd Power x Temp_AutoCLave'] = df_train['2nd Pressure Power_AutoClave'] * df_train['Chamber Temp. Collect Result_AutoClave']\n",
    "df_train['3rd Power x Temp_AutoCLave'] = df_train['3rd Pressure Power_AutoClave'] * df_train['Chamber Temp. Collect Result_AutoClave']\n",
    "\n",
    "df_test['1st Pressure Power_AutoClave'] = (1 - df_test['1st Pressure Collect Result_AutoClave']) * df_test['1st Pressure 1st Pressure Unit Time_AutoClave']\n",
    "df_test['2nd Pressure Power_AutoClave'] = (1 - df_test['2nd Pressure Collect Result_AutoClave']) * df_test['2nd Pressure Unit Time_AutoClave']\n",
    "df_test['3rd Pressure Power_AutoClave'] = (1 - df_test['3rd Pressure Collect Result_AutoClave']) * df_test['3rd Pressure Unit Time_AutoClave']\n",
    "\n",
    "df_test['1st Power x Temp_AutoCLave'] = df_test['1st Pressure Power_AutoClave'] * df_test['Chamber Temp. Collect Result_AutoClave']\n",
    "df_test['2nd Power x Temp_AutoCLave'] = df_test['2nd Pressure Power_AutoClave'] * df_test['Chamber Temp. Collect Result_AutoClave']\n",
    "df_test['3rd Power x Temp_AutoCLave'] = df_test['3rd Pressure Power_AutoClave'] * df_test['Chamber Temp. Collect Result_AutoClave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "id": "9c93f3d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Line 합 구하기\n",
    "df_train['Stage1 Line Sum Speed_Dam'] = df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] + df_train['Stage1 Line2 Distance Speed Collect Result_Dam'] + df_train['Stage1 Line3 Distance Speed Collect Result_Dam'] + df_train['Stage1 Line4 Distance Speed Collect Result_Dam']\n",
    "df_train['Stage2 Line Sum Speed_Dam'] = df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] + df_train['Stage2 Line2 Distance Speed Collect Result_Dam'] + df_train['Stage2 Line3 Distance Speed Collect Result_Dam'] + df_train['Stage2 Line4 Distance Speed Collect Result_Dam']\n",
    "df_train['Stage3 Line Sum Speed_Dam'] = df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] + df_train['Stage3 Line2 Distance Speed Collect Result_Dam'] + df_train['Stage3 Line3 Distance Speed Collect Result_Dam'] + df_train['Stage3 Line4 Distance Speed Collect Result_Dam']\n",
    "\n",
    "df_test['Stage1 Line Sum Speed_Dam'] = df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] + df_test['Stage1 Line2 Distance Speed Collect Result_Dam'] + df_test['Stage1 Line3 Distance Speed Collect Result_Dam'] + df_test['Stage1 Line4 Distance Speed Collect Result_Dam']\n",
    "df_test['Stage2 Line Sum Speed_Dam'] = df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] + df_test['Stage2 Line2 Distance Speed Collect Result_Dam'] + df_test['Stage2 Line3 Distance Speed Collect Result_Dam'] + df_test['Stage2 Line4 Distance Speed Collect Result_Dam']\n",
    "df_test['Stage3 Line Sum Speed_Dam'] = df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] + df_test['Stage3 Line2 Distance Speed Collect Result_Dam'] + df_test['Stage3 Line3 Distance Speed Collect Result_Dam'] + df_test['Stage3 Line4 Distance Speed Collect Result_Dam']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "id": "c29d017c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 온도 x 시간 x 압력\n",
    "df_train['1st Pressure x Time x Temp AutoClave'] = df_train['1st Pressure Collect Result_AutoClave']*df_train['1st Pressure 1st Pressure Unit Time_AutoClave']*df_train['Chamber Temp. Collect Result_AutoClave']\n",
    "df_train['2nd Pressure x Time x Temp AutoClave'] = df_train['2nd Pressure Collect Result_AutoClave']*df_train['2nd Pressure Unit Time_AutoClave']*df_train['Chamber Temp. Collect Result_AutoClave']\n",
    "df_train['3rd Pressure x Time x Temp AutoClave'] = df_train['3rd Pressure Collect Result_AutoClave']*df_train['3rd Pressure Unit Time_AutoClave']*df_train['Chamber Temp. Collect Result_AutoClave']\n",
    "\n",
    "df_test['1st Pressure x Time x Temp AutoClave'] = df_test['1st Pressure Collect Result_AutoClave']*df_test['1st Pressure 1st Pressure Unit Time_AutoClave']*df_test['Chamber Temp. Collect Result_AutoClave']\n",
    "df_test['2nd Pressure x Time x Temp AutoClave'] = df_test['2nd Pressure Collect Result_AutoClave']*df_test['2nd Pressure Unit Time_AutoClave']*df_test['Chamber Temp. Collect Result_AutoClave']\n",
    "df_test['3rd Pressure x Time x Temp AutoClave'] = df_test['3rd Pressure Collect Result_AutoClave']*df_test['3rd Pressure Unit Time_AutoClave']*df_test['Chamber Temp. Collect Result_AutoClave']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "id": "f4b63af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train\n",
    "# Stage별 토출량, 토출 속도, 토출 소요시간 데이터\n",
    "df_train['RESIN Predicted_Volume Stage1 Dam'] = df_train['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df_train['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "df_train['RESIN Predicted_Volume Stage2 Dam'] = df_train['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df_train['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "df_train['RESIN Predicted_Volume Stage3 Dam'] = df_train['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df_train['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "\n",
    "# Stage별 실제 토출량과 예측 토출량의 비율(조정 계수) 계산\n",
    "df_train['Stage1 Scaling_Factor'] = df_train['Dispense Volume(Stage1) Collect Result_Dam'] / df_train['RESIN Predicted_Volume Stage1 Dam']\n",
    "df_train['Stage2 Scaling_Factor'] = df_train['Dispense Volume(Stage2) Collect Result_Dam'] / df_train['RESIN Predicted_Volume Stage2 Dam']\n",
    "df_train['Stage3 Scaling_Factor'] = df_train['Dispense Volume(Stage3) Collect Result_Dam'] / df_train['RESIN Predicted_Volume Stage3 Dam']\n",
    "\n",
    "# Stage별 조정된 예측 토출량 계산\n",
    "df_train['RESIN Adjusted_Predicted_Volume Stage1 Dam'] = df_train['RESIN Predicted_Volume Stage1 Dam'] * df_train['Stage1 Scaling_Factor'].mean()\n",
    "df_train['RESIN Adjusted_Predicted_Volume Stage2 Dam'] = df_train['RESIN Predicted_Volume Stage2 Dam'] * df_train['Stage2 Scaling_Factor'].mean()\n",
    "df_train['RESIN Adjusted_Predicted_Volume Stage3 Dam'] = df_train['RESIN Predicted_Volume Stage3 Dam'] * df_train['Stage3 Scaling_Factor'].mean()\n",
    "\n",
    "## test\n",
    "# Stage별 토출량, 토출 속도, 토출 소요시간 데이터\n",
    "df_test['RESIN Predicted_Volume Stage1 Dam'] = df_test['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df_test['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam']\n",
    "df_test['RESIN Predicted_Volume Stage2 Dam'] = df_test['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df_test['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam']\n",
    "df_test['RESIN Predicted_Volume Stage3 Dam'] = df_test['DISCHARGED SPEED OF RESIN Collect Result_Dam'] * df_test['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam']\n",
    "\n",
    "# Stage별 실제 토출량과 예측 토출량의 비율(조정 계수) 계산\n",
    "df_test['Stage1 Scaling_Factor'] = df_test['Dispense Volume(Stage1) Collect Result_Dam'] / df_test['RESIN Predicted_Volume Stage1 Dam']\n",
    "df_test['Stage2 Scaling_Factor'] = df_test['Dispense Volume(Stage2) Collect Result_Dam'] / df_test['RESIN Predicted_Volume Stage2 Dam']\n",
    "df_test['Stage3 Scaling_Factor'] = df_test['Dispense Volume(Stage3) Collect Result_Dam'] / df_test['RESIN Predicted_Volume Stage3 Dam']\n",
    "\n",
    "# Stage별 조정된 예측 토출량 계산\n",
    "df_test['RESIN Adjusted_Predicted_Volume Stage1 Dam'] = df_test['RESIN Predicted_Volume Stage1 Dam'] * df_test['Stage1 Scaling_Factor'].mean()\n",
    "df_test['RESIN Adjusted_Predicted_Volume Stage2 Dam'] = df_test['RESIN Predicted_Volume Stage2 Dam'] * df_test['Stage2 Scaling_Factor'].mean()\n",
    "df_test['RESIN Adjusted_Predicted_Volume Stage3 Dam'] = df_test['RESIN Predicted_Volume Stage3 Dam'] * df_test['Stage3 Scaling_Factor'].mean()\n",
    "\n",
    "\n",
    "## train\n",
    "# Stage별 토출량, 토출 속도, 토출 소요시간 데이터\n",
    "df_train['RESIN Predicted_Volume Stage1 Fill1'] = df_train['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df_train['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1']\n",
    "df_train['RESIN Predicted_Volume Stage2 Fill1'] = df_train['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df_train['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1']\n",
    "df_train['RESIN Predicted_Volume Stage3 Fill1'] = df_train['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df_train['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']\n",
    "\n",
    "# Stage별 실제 토출량과 예측 토출량의 비율(조정 계수) 계산\n",
    "df_train['Stage1 Scaling_Factor'] = df_train['Dispense Volume(Stage1) Collect Result_Fill1'] / df_train['RESIN Predicted_Volume Stage1 Fill1']\n",
    "df_train['Stage2 Scaling_Factor'] = df_train['Dispense Volume(Stage2) Collect Result_Fill1'] / df_train['RESIN Predicted_Volume Stage2 Fill1']\n",
    "df_train['Stage3 Scaling_Factor'] = df_train['Dispense Volume(Stage3) Collect Result_Fill1'] / df_train['RESIN Predicted_Volume Stage3 Fill1']\n",
    "\n",
    "# Stage별 조정된 예측 토출량 계산\n",
    "df_train['RESIN Adjusted_Predicted_Volume Stage1 Fill1'] = df_train['RESIN Predicted_Volume Stage1 Fill1'] * df_train['Stage1 Scaling_Factor'].mean()\n",
    "df_train['RESIN Adjusted_Predicted_Volume Stage2 Fill1'] = df_train['RESIN Predicted_Volume Stage2 Fill1'] * df_train['Stage2 Scaling_Factor'].mean()\n",
    "df_train['RESIN Adjusted_Predicted_Volume Stage3 Fill1'] = df_train['RESIN Predicted_Volume Stage3 Fill1'] * df_train['Stage3 Scaling_Factor'].mean()\n",
    "\n",
    "## test\n",
    "# Stage별 토출량, 토출 속도, 토출 소요시간 데이터\n",
    "df_test['RESIN Predicted_Volume Stage1 Fill1'] = df_test['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df_test['DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1']\n",
    "df_test['RESIN Predicted_Volume Stage2 Fill1'] = df_test['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df_test['DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1']\n",
    "df_test['RESIN Predicted_Volume Stage3 Fill1'] = df_test['DISCHARGED SPEED OF RESIN Collect Result_Fill1'] * df_test['DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1']\n",
    "\n",
    "# Stage별 실제 토출량과 예측 토출량의 비율(조정 계수) 계산\n",
    "df_test['Stage1 Scaling_Factor'] = df_test['Dispense Volume(Stage1) Collect Result_Fill1'] / df_test['RESIN Predicted_Volume Stage1 Fill1']\n",
    "df_test['Stage2 Scaling_Factor'] = df_test['Dispense Volume(Stage2) Collect Result_Fill1'] / df_test['RESIN Predicted_Volume Stage2 Fill1']\n",
    "df_test['Stage3 Scaling_Factor'] = df_test['Dispense Volume(Stage3) Collect Result_Fill1'] / df_test['RESIN Predicted_Volume Stage3 Fill1']\n",
    "\n",
    "# Stage별 조정된 예측 토출량 계산\n",
    "df_test['RESIN Adjusted_Predicted_Volume Stage1 Fill1'] = df_test['RESIN Predicted_Volume Stage1 Fill1'] * df_test['Stage1 Scaling_Factor'].mean()\n",
    "df_test['RESIN Adjusted_Predicted_Volume Stage2 Fill1'] = df_test['RESIN Predicted_Volume Stage2 Fill1'] * df_test['Stage2 Scaling_Factor'].mean()\n",
    "df_test['RESIN Adjusted_Predicted_Volume Stage3 Fill1'] = df_test['RESIN Predicted_Volume Stage3 Fill1'] * df_test['Stage3 Scaling_Factor'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "id": "a30f4585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# workorder 6자만 분리\n",
    "df_train['workorder_for'] = df_train['Workorder_Dam'].str.slice(0, 6)\n",
    "df_test['workorder_for'] = df_test['Workorder_Dam'].str.slice(0, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "d2c14021",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tact time 합산\n",
    "df_train['All Tact Time'] = df_train['Machine Tact time Collect Result_Dam'] + df_train['Chamber Temp. Unit Time_AutoClave'] + df_train['Machine Tact time Collect Result_Fill1'] + df_train['Machine Tact time Collect Result_Fill2']\n",
    "df_test['All Tact Time'] = df_test['Machine Tact time Collect Result_Dam'] + df_test['Chamber Temp. Unit Time_AutoClave'] + df_test['Machine Tact time Collect Result_Fill1'] + df_test['Machine Tact time Collect Result_Fill2']\n",
    "\n",
    "df_train['Fill Time'] = df_train['Machine Tact time Collect Result_Fill1'] + df_train['Machine Tact time Collect Result_Fill2']\n",
    "df_test['Fill Time'] = df_test['Machine Tact time Collect Result_Fill1'] + df_test['Machine Tact time Collect Result_Fill2']\n",
    "\n",
    "df_train['Dam Fill1 Time'] = df_train['Machine Tact time Collect Result_Dam'] + df_train['Machine Tact time Collect Result_Fill1']\n",
    "df_test['Dam Fill1 Time'] = df_train['Machine Tact time Collect Result_Dam'] + df_test['Machine Tact time Collect Result_Fill1']\n",
    "\n",
    "df_train['Dam Fill2 Time'] = df_train['Machine Tact time Collect Result_Dam'] + df_train['Machine Tact time Collect Result_Fill2']\n",
    "df_test['Dam Fill2 Time'] = df_train['Machine Tact time Collect Result_Dam'] + df_test['Machine Tact time Collect Result_Fill2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "id": "0234386c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in ['Collect Date_Dam', 'Collect Date_Fill1', 'Collect Date_Fill2', 'Collect Date_AutoClave']:\n",
    "    df_train[column] = pd.to_datetime(df_train[column])\n",
    "\n",
    "for column in ['Collect Date_Dam', 'Collect Date_Fill1', 'Collect Date_Fill2', 'Collect Date_AutoClave']:\n",
    "    df_test[column] = pd.to_datetime(df_test[column])\n",
    "    \n",
    "df_train['Collect Date'] = pd.to_datetime(df_train['Collect Date_Dam'].dt.date)\n",
    "df_test['Collect Date'] = pd.to_datetime(df_test['Collect Date_Dam'].dt.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "id": "850c96fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10088/1091887417.py:5: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['Hour_Dam'] = df_train['Collect Date_Dam'].dt.hour\n",
      "/tmp/ipykernel_10088/1091887417.py:6: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['Minute_Dam'] = df_train['Collect Date_Dam'].dt.minute\n",
      "/tmp/ipykernel_10088/1091887417.py:8: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['Hour_Fill1'] = df_train['Collect Date_Fill1'].dt.hour\n",
      "/tmp/ipykernel_10088/1091887417.py:9: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['Minute_Fill1'] = df_train['Collect Date_Fill1'].dt.minute\n",
      "/tmp/ipykernel_10088/1091887417.py:11: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['Hour_Fill2'] = df_train['Collect Date_Fill2'].dt.hour\n",
      "/tmp/ipykernel_10088/1091887417.py:12: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['Minute_Fill2'] = df_train['Collect Date_Fill2'].dt.minute\n",
      "/tmp/ipykernel_10088/1091887417.py:14: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['Hour_AutoClave'] = df_train['Collect Date_AutoClave'].dt.hour\n",
      "/tmp/ipykernel_10088/1091887417.py:15: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['Minute_AutoClave'] = df_train['Collect Date_AutoClave'].dt.minute\n",
      "/tmp/ipykernel_10088/1091887417.py:21: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['Hour_Dam'] = df_test['Collect Date_Dam'].dt.hour\n",
      "/tmp/ipykernel_10088/1091887417.py:22: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['Minute_Dam'] = df_test['Collect Date_Dam'].dt.minute\n",
      "/tmp/ipykernel_10088/1091887417.py:24: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['Hour_Fill1'] = df_test['Collect Date_Fill1'].dt.hour\n",
      "/tmp/ipykernel_10088/1091887417.py:25: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['Minute_Fill1'] = df_test['Collect Date_Fill1'].dt.minute\n",
      "/tmp/ipykernel_10088/1091887417.py:27: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['Hour_Fill2'] = df_test['Collect Date_Fill2'].dt.hour\n",
      "/tmp/ipykernel_10088/1091887417.py:28: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['Minute_Fill2'] = df_test['Collect Date_Fill2'].dt.minute\n",
      "/tmp/ipykernel_10088/1091887417.py:30: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['Hour_AutoClave'] = df_test['Collect Date_AutoClave'].dt.hour\n",
      "/tmp/ipykernel_10088/1091887417.py:31: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['Minute_AutoClave'] = df_test['Collect Date_AutoClave'].dt.minute\n"
     ]
    }
   ],
   "source": [
    "df_train['Year'] = df_train['Collect Date'].dt.year\n",
    "df_train['Month'] = df_train['Collect Date'].dt.month\n",
    "df_train['Day'] = df_train['Collect Date'].dt.day\n",
    "\n",
    "df_train['Hour_Dam'] = df_train['Collect Date_Dam'].dt.hour\n",
    "df_train['Minute_Dam'] = df_train['Collect Date_Dam'].dt.minute\n",
    "\n",
    "df_train['Hour_Fill1'] = df_train['Collect Date_Fill1'].dt.hour\n",
    "df_train['Minute_Fill1'] = df_train['Collect Date_Fill1'].dt.minute\n",
    "\n",
    "df_train['Hour_Fill2'] = df_train['Collect Date_Fill2'].dt.hour\n",
    "df_train['Minute_Fill2'] = df_train['Collect Date_Fill2'].dt.minute\n",
    "\n",
    "df_train['Hour_AutoClave'] = df_train['Collect Date_AutoClave'].dt.hour\n",
    "df_train['Minute_AutoClave'] = df_train['Collect Date_AutoClave'].dt.minute\n",
    "\n",
    "df_test['Year'] = df_test['Collect Date'].dt.year\n",
    "df_test['Month'] = df_test['Collect Date'].dt.month\n",
    "df_test['Day'] = df_test['Collect Date'].dt.day\n",
    "\n",
    "df_test['Hour_Dam'] = df_test['Collect Date_Dam'].dt.hour\n",
    "df_test['Minute_Dam'] = df_test['Collect Date_Dam'].dt.minute\n",
    "\n",
    "df_test['Hour_Fill1'] = df_test['Collect Date_Fill1'].dt.hour\n",
    "df_test['Minute_Fill1'] = df_test['Collect Date_Fill1'].dt.minute\n",
    "\n",
    "df_test['Hour_Fill2'] = df_test['Collect Date_Fill2'].dt.hour\n",
    "df_test['Minute_Fill2'] = df_test['Collect Date_Fill2'].dt.minute\n",
    "\n",
    "df_test['Hour_AutoClave'] = df_test['Collect Date_AutoClave'].dt.hour\n",
    "df_test['Minute_AutoClave'] = df_test['Collect Date_AutoClave'].dt.minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "id": "659884e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = ['Year',\n",
    "'Month',\n",
    "'Day',\n",
    "'Hour_Dam',\n",
    "'Minute_Dam',\n",
    "'Hour_Fill1',\n",
    "'Minute_Fill1',\n",
    "'Hour_Fill2',\n",
    "'Minute_Fill2',\n",
    "'Hour_AutoClave',\n",
    "'Minute_AutoClave']\n",
    "\n",
    "for i in c:\n",
    "    df_train[i] = df_train[i].astype(float)\n",
    "    df_test[i] = df_test[i].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "id": "19663ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10088/2923948295.py:1: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['timediff_autoclave_fill2'] = (df_train['Collect Date_AutoClave'] - df_train['Collect Date_Fill2']).dt.total_seconds() // 60\n",
      "/tmp/ipykernel_10088/2923948295.py:2: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['timediff_autoclave_fill2'] = (df_test['Collect Date_AutoClave'] - df_test['Collect Date_Fill2']).dt.total_seconds() // 60\n"
     ]
    }
   ],
   "source": [
    "df_train['timediff_autoclave_fill2'] = (df_train['Collect Date_AutoClave'] - df_train['Collect Date_Fill2']).dt.total_seconds() // 60\n",
    "df_test['timediff_autoclave_fill2'] = (df_test['Collect Date_AutoClave'] - df_test['Collect Date_Fill2']).dt.total_seconds() // 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "575d027d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10088/2251481481.py:4: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_train['DateTime'] = pd.to_datetime(df_train['Collect Date_Dam'].apply(lambda x: x.replace(minute=0)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                DateTime  Temperature  Humidity\n",
      "0    2023-05-04 00:00:00         25.2        96\n",
      "1    2023-05-04 01:00:00         25.1        96\n",
      "2    2023-05-04 02:00:00         25.1        96\n",
      "3    2023-05-04 03:00:00         25.1        96\n",
      "4    2023-05-04 04:00:00         25.3        95\n",
      "...                  ...          ...       ...\n",
      "8659 2024-04-28 19:00:00         29.1        81\n",
      "8660 2024-04-28 20:00:00         28.1        87\n",
      "8661 2024-04-28 21:00:00         27.8        88\n",
      "8662 2024-04-28 22:00:00         27.6        88\n",
      "8663 2024-04-28 23:00:00         27.5        88\n",
      "\n",
      "[8664 rows x 3 columns]\n",
      "      Wip Line_Dam Process Desc._Dam Model.Suffix_Dam Workorder_Dam  \\\n",
      "0          IVI-OB6     Dam Dispenser      AJX75334505    4F1XA938-1   \n",
      "1          IVI-OB6     Dam Dispenser      AJX75334505    3KPM0016-2   \n",
      "2          IVI-OB6     Dam Dispenser      AJX75334501    4E1X9167-1   \n",
      "3          IVI-OB6     Dam Dispenser      AJX75334501    3K1X0057-1   \n",
      "4          IVI-OB6     Dam Dispenser      AJX75334501    3HPM0007-1   \n",
      "...            ...               ...              ...           ...   \n",
      "40501      IVI-OB6     Dam Dispenser      AJX75334501    3J1XF434-2   \n",
      "40502      IVI-OB6     Dam Dispenser      AJX75334501    4E1XC796-1   \n",
      "40503      IVI-OB6     Dam Dispenser      AJX75334501    4C1XD438-1   \n",
      "40504      IVI-OB6     Dam Dispenser      AJX75334501    3I1XA258-1   \n",
      "40505      IVI-OB6     Dam Dispenser      AJX75334501    3G1XA501-1   \n",
      "\n",
      "         Collect Date_Dam  Insp. Seq No._Dam Insp Judge Code_Dam  \\\n",
      "0     2024-04-25 11:10:00                  1                  OK   \n",
      "1     2023-09-19 14:30:00                  1                  OK   \n",
      "2     2024-03-05 09:30:00                  1                  OK   \n",
      "3     2023-09-25 15:40:00                  1                  OK   \n",
      "4     2023-06-27 13:20:00                  1                  OK   \n",
      "...                   ...                ...                 ...   \n",
      "40501 2023-09-14 15:50:00                  1                  OK   \n",
      "40502 2024-04-10 14:20:00                  1                  OK   \n",
      "40503 2024-02-22 08:40:00                  1                  OK   \n",
      "40504 2023-07-25 11:00:00                  1                  OK   \n",
      "40505 2023-05-31 01:40:00                  1                  OK   \n",
      "\n",
      "       CURE END POSITION X Collect Result_Dam  \\\n",
      "0                                       240.0   \n",
      "1                                       240.0   \n",
      "2                                      1000.0   \n",
      "3                                      1000.0   \n",
      "4                                       240.0   \n",
      "...                                       ...   \n",
      "40501                                   240.0   \n",
      "40502                                  1000.0   \n",
      "40503                                   240.0   \n",
      "40504                                  1000.0   \n",
      "40505                                   240.0   \n",
      "\n",
      "       CURE END POSITION Z Collect Result_Dam  \\\n",
      "0                                         2.5   \n",
      "1                                         2.5   \n",
      "2                                        12.5   \n",
      "3                                        12.5   \n",
      "4                                         2.5   \n",
      "...                                       ...   \n",
      "40501                                     2.5   \n",
      "40502                                    12.5   \n",
      "40503                                     2.5   \n",
      "40504                                    12.5   \n",
      "40505                                     2.5   \n",
      "\n",
      "       CURE END POSITION Θ Collect Result_Dam  ...  Hour_Fill1  Minute_Fill1  \\\n",
      "0                                         -90  ...        11.0          20.0   \n",
      "1                                         -90  ...        14.0          30.0   \n",
      "2                                          90  ...         9.0          30.0   \n",
      "3                                          90  ...        15.0          40.0   \n",
      "4                                         -90  ...        13.0          20.0   \n",
      "...                                       ...  ...         ...           ...   \n",
      "40501                                     -90  ...        15.0          50.0   \n",
      "40502                                      90  ...        14.0          20.0   \n",
      "40503                                     -90  ...         8.0          50.0   \n",
      "40504                                      90  ...        11.0           0.0   \n",
      "40505                                     -90  ...         1.0          40.0   \n",
      "\n",
      "       Hour_Fill2  Minute_Fill2  Hour_AutoClave  Minute_AutoClave  \\\n",
      "0            11.0          20.0            11.0              50.0   \n",
      "1            14.0          30.0            15.0               0.0   \n",
      "2             9.0          30.0            10.0              10.0   \n",
      "3            15.0          40.0            16.0              20.0   \n",
      "4            13.0          20.0            14.0               0.0   \n",
      "...           ...           ...             ...               ...   \n",
      "40501        15.0          50.0            16.0              30.0   \n",
      "40502        14.0          20.0            15.0               0.0   \n",
      "40503         8.0          50.0             9.0              20.0   \n",
      "40504        11.0           0.0            11.0              40.0   \n",
      "40505         1.0          40.0             2.0              10.0   \n",
      "\n",
      "       timediff_autoclave_fill2            DateTime  Temperature  Humidity  \n",
      "0                          30.0 2024-04-25 11:00:00         31.9        71  \n",
      "1                          30.0 2023-09-19 14:00:00         30.4        75  \n",
      "2                          40.0 2024-03-05 09:00:00         25.9        78  \n",
      "3                          40.0 2023-09-25 15:00:00         29.3        77  \n",
      "4                          40.0 2023-06-27 13:00:00         29.6        76  \n",
      "...                         ...                 ...          ...       ...  \n",
      "40501                      40.0 2023-09-14 15:00:00         26.8        83  \n",
      "40502                      40.0 2024-04-10 14:00:00         27.1        75  \n",
      "40503                      30.0 2024-02-22 08:00:00         23.9        91  \n",
      "40504                      40.0 2023-07-25 11:00:00         32.4        65  \n",
      "40505                      30.0 2023-05-31 01:00:00         26.3        95  \n",
      "\n",
      "[40506 rows x 254 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10088/2251481481.py:20: PerformanceWarning: DataFrame is highly fragmented.  This is usually the result of calling `frame.insert` many times, which has poor performance.  Consider joining all columns at once using pd.concat(axis=1) instead. To get a de-fragmented frame, use `newframe = frame.copy()`\n",
      "  df_test['DateTime'] = pd.to_datetime(df_test['Collect Date_Dam'].apply(lambda x: x.replace(minute=0)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                DateTime  Temperature  Humidity\n",
      "0    2023-05-04 00:00:00         25.2        96\n",
      "1    2023-05-04 01:00:00         25.1        96\n",
      "2    2023-05-04 02:00:00         25.1        96\n",
      "3    2023-05-04 03:00:00         25.1        96\n",
      "4    2023-05-04 04:00:00         25.3        95\n",
      "...                  ...          ...       ...\n",
      "8659 2024-04-28 19:00:00         29.1        81\n",
      "8660 2024-04-28 20:00:00         28.1        87\n",
      "8661 2024-04-28 21:00:00         27.8        88\n",
      "8662 2024-04-28 22:00:00         27.6        88\n",
      "8663 2024-04-28 23:00:00         27.5        88\n",
      "\n",
      "[8664 rows x 3 columns]\n",
      "      Wip Line_Dam Process Desc._Dam Model.Suffix_Dam Workorder_Dam  \\\n",
      "0          IVI-OB6     Dam Dispenser      AJX75334501    3J1XF767-1   \n",
      "1          IVI-OB6     Dam Dispenser      AJX75334501    4B1XD472-2   \n",
      "2          IVI-OB6     Dam Dispenser      AJX75334501    3H1XE355-1   \n",
      "3          IVI-OB6     Dam Dispenser      AJX75334501    3L1XA128-1   \n",
      "4          IVI-OB6     Dam Dispenser      AJX75334501    4A1XA639-1   \n",
      "...            ...               ...              ...           ...   \n",
      "17356      IVI-OB6     Dam Dispenser      AJX75334501    3K1XB597-1   \n",
      "17357      IVI-OB6     Dam Dispenser      AJX75334501    4A1XB974-1   \n",
      "17358      IVI-OB6     Dam Dispenser      AJX75334501    3L1XA998-1   \n",
      "17359      IVI-OB6     Dam Dispenser      AJX75334501    3F1XC376-1   \n",
      "17360      IVI-OB6     Dam Dispenser      AJX75334501    3J1XC756-1   \n",
      "\n",
      "         Collect Date_Dam  Insp. Seq No._Dam Insp Judge Code_Dam  \\\n",
      "0     2023-09-15 13:20:00                  1                  OK   \n",
      "1     2024-02-06 16:50:00                  1                  OK   \n",
      "2     2023-07-14 11:30:00                  1                  OK   \n",
      "3     2023-11-03 08:00:00                  1                  OK   \n",
      "4     2023-12-23 14:00:00                  1                  OK   \n",
      "...                   ...                ...                 ...   \n",
      "17356 2023-10-18 11:10:00                  1                  OK   \n",
      "17357 2024-01-02 15:00:00                  1                  OK   \n",
      "17358 2023-11-15 09:10:00                  1                  OK   \n",
      "17359 2023-05-18 14:20:00                  1                  OK   \n",
      "17360 2023-08-23 09:30:00                  1                  OK   \n",
      "\n",
      "       CURE END POSITION X Collect Result_Dam  \\\n",
      "0                                      1000.0   \n",
      "1                                      1000.0   \n",
      "2                                       240.0   \n",
      "3                                      1000.0   \n",
      "4                                       240.0   \n",
      "...                                       ...   \n",
      "17356                                  1000.0   \n",
      "17357                                  1000.0   \n",
      "17358                                   240.0   \n",
      "17359                                   240.0   \n",
      "17360                                   240.0   \n",
      "\n",
      "       CURE END POSITION Z Collect Result_Dam  \\\n",
      "0                                        12.5   \n",
      "1                                        12.5   \n",
      "2                                         2.5   \n",
      "3                                        12.5   \n",
      "4                                         2.5   \n",
      "...                                       ...   \n",
      "17356                                    12.5   \n",
      "17357                                    12.5   \n",
      "17358                                     2.5   \n",
      "17359                                     2.5   \n",
      "17360                                     2.5   \n",
      "\n",
      "       CURE END POSITION Θ Collect Result_Dam  ...  Hour_Fill1  Minute_Fill1  \\\n",
      "0                                          90  ...        13.0          30.0   \n",
      "1                                          90  ...        16.0          50.0   \n",
      "2                                         -90  ...        11.0          40.0   \n",
      "3                                          90  ...         8.0           0.0   \n",
      "4                                         -90  ...        14.0           0.0   \n",
      "...                                       ...  ...         ...           ...   \n",
      "17356                                      90  ...        11.0          10.0   \n",
      "17357                                      90  ...        15.0           0.0   \n",
      "17358                                     -90  ...         9.0          10.0   \n",
      "17359                                     -90  ...        14.0          20.0   \n",
      "17360                                     -90  ...         9.0          30.0   \n",
      "\n",
      "       Hour_Fill2  Minute_Fill2  Hour_AutoClave  Minute_AutoClave  \\\n",
      "0            13.0          30.0            14.0               0.0   \n",
      "1            16.0          50.0            17.0              30.0   \n",
      "2            11.0          40.0            12.0              10.0   \n",
      "3             8.0           0.0             8.0              30.0   \n",
      "4            14.0          10.0            14.0              40.0   \n",
      "...           ...           ...             ...               ...   \n",
      "17356        11.0          10.0            11.0              50.0   \n",
      "17357        15.0           0.0            15.0              30.0   \n",
      "17358         9.0          10.0             9.0              40.0   \n",
      "17359        14.0          20.0            14.0              50.0   \n",
      "17360         9.0          30.0            10.0               0.0   \n",
      "\n",
      "       timediff_autoclave_fill2            DateTime  Temperature  Humidity  \n",
      "0                          30.0 2023-09-15 13:00:00         26.9        86  \n",
      "1                          40.0 2024-02-06 16:00:00         21.2        82  \n",
      "2                          30.0 2023-07-14 11:00:00         33.4        69  \n",
      "3                          30.0 2023-11-03 08:00:00         26.6        83  \n",
      "4                          30.0 2023-12-23 14:00:00         17.4        34  \n",
      "...                         ...                 ...          ...       ...  \n",
      "17356                      40.0 2023-10-18 11:00:00         30.5        58  \n",
      "17357                      30.0 2024-01-02 15:00:00         25.2        70  \n",
      "17358                      30.0 2023-11-15 09:00:00         21.6        71  \n",
      "17359                      30.0 2023-05-18 14:00:00         36.6        47  \n",
      "17360                      30.0 2023-08-23 09:00:00         30.4        81  \n",
      "\n",
      "[17361 rows x 254 columns]\n"
     ]
    }
   ],
   "source": [
    "# 기상 데이터 가져오기 (여러 번 요청)\n",
    "from datetime import datetime\n",
    "\n",
    "df_train['DateTime'] = pd.to_datetime(df_train['Collect Date_Dam'].apply(lambda x: x.replace(minute=0)))\n",
    "start_date = datetime.strptime('2023-05-04', '%Y-%m-%d').date()\n",
    "end_date = datetime.strptime('2024-04-28', '%Y-%m-%d').date()\n",
    "\n",
    "weather_data = fetch_weather_data(LATITUDE, LONGITUDE, start_date, end_date)\n",
    "print(weather_data)\n",
    "\n",
    "if weather_data is not None:\n",
    "\n",
    "    # 병합\n",
    "    df_train = pd.merge(df_train, weather_data, on='DateTime', how='left')\n",
    "\n",
    "    # 결과 확인\n",
    "    print(df_train)\n",
    "    \n",
    "    \n",
    "df_test['DateTime'] = pd.to_datetime(df_test['Collect Date_Dam'].apply(lambda x: x.replace(minute=0)))\n",
    "start_date = datetime.strptime('2023-05-04', '%Y-%m-%d').date()\n",
    "end_date = datetime.strptime('2024-04-28', '%Y-%m-%d').date()\n",
    "\n",
    "weather_data = fetch_weather_data(LATITUDE, LONGITUDE, start_date, end_date)\n",
    "print(weather_data)\n",
    "\n",
    "if weather_data is not None:\n",
    "\n",
    "    # 병합\n",
    "    df_test = pd.merge(df_test, weather_data, on='DateTime', how='left')\n",
    "\n",
    "    # 결과 확인\n",
    "    print(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c76263",
   "metadata": {},
   "source": [
    "- 칼럼명 변경 및 필요없는 데이터 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "38b6d1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.drop(columns= [ \n",
    " 'Stage1 Circle2 Distance Speed Collect Result_Dam',\n",
    " 'Stage1 Circle3 Distance Speed Collect Result_Dam',\n",
    " 'Stage1 Circle4 Distance Speed Collect Result_Dam', \n",
    " 'Stage2 Circle2 Distance Speed Collect Result_Dam',\n",
    " 'Stage2 Circle3 Distance Speed Collect Result_Dam',\n",
    " 'Stage2 Circle4 Distance Speed Collect Result_Dam', \n",
    " 'Stage3 Circle2 Distance Speed Collect Result_Dam',\n",
    " 'Stage3 Circle3 Distance Speed Collect Result_Dam',\n",
    " 'Stage3 Circle4 Distance Speed Collect Result_Dam'] )\n",
    "\n",
    "df_test = df_test.rename(columns={'Stage1 Circle1 Distance Speed Collect Result_Dam': 'Stage1 Circle Distance Speed_Dam', \n",
    "                                    'Stage2 Circle1 Distance Speed Collect Result_Dam': 'Stage2 Circle Distance Speed_Dam',\n",
    "                                    'Stage3 Circle1 Distance Speed Collect Result_Dam': 'Stage3 Circle Distance Speed_Dam'})\n",
    "\n",
    "# Dam, Fill2의 경우 Z값이 서로 같다. -> 그렇다면 Fill1은 높이값에서 흔들린 경우가 있다는 것을 의미한다.\n",
    "df_test = df_test.drop(columns= [\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2'\n",
    "])\n",
    "\n",
    "df_test = df_test.rename(columns={'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2': 'HEAD NORMAL COORDINATE Z AXIS_Fill2', \n",
    "                                    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam': 'HEAD NORMAL COORDINATE Z AXIS_Dam',\n",
    "                                    })\n",
    "\n",
    "# Model.Suffix, Workorder이 같다.\n",
    "df_test = df_test.drop(columns=['Model.Suffix_Fill1', 'Model.Suffix_Fill2', 'Model.Suffix_AutoClave'])\n",
    "df_test = df_test.drop(columns=['Workorder_Fill1', 'Workorder_Fill2', 'Workorder_AutoClave'])\n",
    "df_test = df_test.rename(columns={'Workorder_Dam': 'Workorder', 'Model.Suffix_Dam': 'Model.Suffix'})\n",
    "\n",
    "# 의미를 찾을 수 없는 컬럼들 제거\n",
    "df_test = df_test.drop(columns=['Wip Line_Fill1', \n",
    "                                  'Process Desc._Fill1', \n",
    "                                  'Insp. Seq No._Fill1', \n",
    "                                  'Insp Judge Code_Fill1', \n",
    "                                  'Equipment_AutoClave',\n",
    "                                  'Process Desc._AutoClave', \n",
    "                                  'Wip Line_AutoClave', \n",
    "                                  'Insp Judge Code_AutoClave',\n",
    "                                  'Insp. Seq No._AutoClave',\n",
    "                                  '1st Pressure Judge Value_AutoClave', \n",
    "                                  '2nd Pressure Judge Value_AutoClave', \n",
    "                                  '3rd Pressure Judge Value_AutoClave', \n",
    "                                  'GMES_ORIGIN_INSP_JUDGE_CODE Collect Result_AutoClave',\n",
    "                                  'GMES_ORIGIN_INSP_JUDGE_CODE Judge Value_AutoClave',\n",
    "                                  'GMES_ORIGIN_INSP_JUDGE_CODE Unit Time_AutoClave',\n",
    "                                  'Wip Line_Fill2', \n",
    "                                  'Process Desc._Fill2', \n",
    "                                  'Insp. Seq No._Fill2', \n",
    "                                  'Insp Judge Code_Fill2', \n",
    "                                  'Wip Line_Dam', \n",
    "                                  'Process Desc._Dam', \n",
    "                                  'Insp. Seq No._Dam', \n",
    "                                  'Insp Judge Code_Dam',\n",
    "                                  'CURE END POSITION X Collect Result_Dam',\n",
    "                                  'CURE END POSITION Z Collect Result_Dam',\n",
    "                                  'CURE END POSITION Θ Collect Result_Dam',\n",
    "                                  'CURE STANDBY POSITION X Collect Result_Dam',\n",
    "                                  'CURE STANDBY POSITION Z Collect Result_Dam',\n",
    "                                  'CURE STANDBY POSITION Θ Collect Result_Dam',\n",
    "                                  ])  \n",
    "\n",
    "# Fill2는 레진을 살포하지 않는다. UV만 진행하는 과정이므로 싹 삭제해 준다.          \n",
    "df_test = df_test.drop(columns=['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2',\n",
    "                                'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2',\n",
    "                                'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2',\n",
    "                                'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2',\n",
    "                                'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2',\n",
    "                                'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2',\n",
    "                                'HEAD NORMAL COORDINATE Z AXIS_Fill2',\n",
    "                                'HEAD Standby Position X Collect Result_Fill2',\n",
    "                                'HEAD Standby Position Y Collect Result_Fill2',\n",
    "                                'HEAD Standby Position Z Collect Result_Fill2',\n",
    "                                'Head Clean Position X Collect Result_Fill2',\n",
    "                                'Head Clean Position Y Collect Result_Fill2',\n",
    "                                'Head Clean Position Z Collect Result_Fill2',\n",
    "                                'Head Purge Position X Collect Result_Fill2',\n",
    "                                'Head Purge Position Y Collect Result_Fill2',\n",
    "                                'Head Purge Position Z Collect Result_Fill2',\n",
    "                                'DISCHARGED SPEED OF RESIN Collect Result_Fill2',\n",
    "                                'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill2',\n",
    "                                'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill2',\n",
    "                                'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill2',\n",
    "                                'Dispense Volume(Stage1) Collect Result_Fill2',\n",
    "                                'Dispense Volume(Stage2) Collect Result_Fill2',\n",
    "                                'Dispense Volume(Stage3) Collect Result_Fill2',])  \n",
    "\n",
    "# # 라인별로 속도가 같아야 정상이다.\n",
    "df_test['Stage1 Line diffent Distance Speed_Dam'] = ((df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage1 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage1 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage1 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage1 Line3 Distance Speed Collect Result_Dam'] != df_test['Stage1 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "\n",
    "df_test['Stage2 Line diffent Distance Speed_Dam'] = ((df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage2 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage2 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage2 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage2 Line3 Distance Speed Collect Result_Dam'] != df_test['Stage2 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "\n",
    "df_test['Stage3 Line diffent Distance Speed_Dam'] = ((df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage3 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage3 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_test['Stage3 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_test['Stage3 Line3 Distance Speed Collect Result_Dam'] != df_test['Stage3 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "\n",
    "\n",
    "# 단일값이 하나인 컬럼들, 의미를 찾고싶다면 주석처리 해야하는 것들\n",
    "df_test = df_test.drop(columns=['CURE START POSITION X Collect Result_Dam', # Equipment에 따라서 정해지며, 하나로 책정됨.\n",
    "                                'CURE START POSITION Z Collect Result_Dam', # START POSITION\n",
    "                                'CURE START POSITION Θ Collect Result_Dam', # Equipment에 따라서 정해지며, 하나로 책정됨.\n",
    "                                'HEAD Standby Position X Collect Result_Dam',\n",
    "                                'HEAD Standby Position Y Collect Result_Dam',\n",
    "                                'HEAD Standby Position Z Collect Result_Dam',\n",
    "                                'Head Clean Position X Collect Result_Dam',\n",
    "                                'Head Clean Position Y Collect Result_Dam', # 흔들림에 따라 Z\n",
    "                                'Head Purge Position X Collect Result_Dam',\n",
    "                                'Head Purge Position Y Collect Result_Dam',\n",
    "                                'Head Zero Position X Collect Result_Dam',\n",
    "                                'HEAD Standby Position X Collect Result_Fill1',\n",
    "                                'HEAD Standby Position Y Collect Result_Fill1',\n",
    "                                'HEAD Standby Position Z Collect Result_Fill1',\n",
    "                                'Head Clean Position X Collect Result_Fill1',\n",
    "                                'Head Clean Position Y Collect Result_Fill1',\n",
    "                                'Head Clean Position Z Collect Result_Fill1',\n",
    "                                'Head Purge Position X Collect Result_Fill1',\n",
    "                                'Head Purge Position Y Collect Result_Fill1',\n",
    "                                'CURE END POSITION X Collect Result_Fill2',\n",
    "                                'CURE END POSITION Θ Collect Result_Fill2',\n",
    "                                'CURE STANDBY POSITION X Collect Result_Fill2',\n",
    "                                'CURE STANDBY POSITION Z Collect Result_Fill2',\n",
    "                                'CURE STANDBY POSITION Θ Collect Result_Fill2',\n",
    "                                'CURE START POSITION X Collect Result_Fill2',\n",
    "                                'CURE START POSITION Θ Collect Result_Fill2',\n",
    "                                ])\n",
    "\n",
    "\n",
    "# df_test = df_test.drop(columns = [ 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',\n",
    "#                                 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "id": "2734146c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 값들이 같은 컬럼 하나로 합치는 과정\n",
    "# 같은 Stage에 Circle 값들끼리 같다.\n",
    "df_train = df_train.drop(columns= [ \n",
    " 'Stage1 Circle2 Distance Speed Collect Result_Dam',\n",
    " 'Stage1 Circle3 Distance Speed Collect Result_Dam',\n",
    " 'Stage1 Circle4 Distance Speed Collect Result_Dam', \n",
    " 'Stage2 Circle2 Distance Speed Collect Result_Dam',\n",
    " 'Stage2 Circle3 Distance Speed Collect Result_Dam',\n",
    " 'Stage2 Circle4 Distance Speed Collect Result_Dam', \n",
    " 'Stage3 Circle2 Distance Speed Collect Result_Dam',\n",
    " 'Stage3 Circle3 Distance Speed Collect Result_Dam',\n",
    " 'Stage3 Circle4 Distance Speed Collect Result_Dam'] )\n",
    "\n",
    "df_train = df_train.rename(columns={'Stage1 Circle1 Distance Speed Collect Result_Dam': 'Stage1 Circle Distance Speed_Dam', \n",
    "                                    'Stage2 Circle1 Distance Speed Collect Result_Dam': 'Stage2 Circle Distance Speed_Dam',\n",
    "                                    'Stage3 Circle1 Distance Speed Collect Result_Dam': 'Stage3 Circle Distance Speed_Dam'})\n",
    "\n",
    "# Dam, Fill2의 경우 Z값이 서로 같다. -> 그렇다면 Fill1은 높이값에서 흔들린 경우가 있다는 것을 의미한다.\n",
    "df_train = df_train.drop(columns= [\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Dam',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill2',\n",
    "    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill2'\n",
    "])\n",
    "\n",
    "df_train = df_train.rename(columns={'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill2': 'HEAD NORMAL COORDINATE Z AXIS_Fill2', \n",
    "                                    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Dam': 'HEAD NORMAL COORDINATE Z AXIS_Dam',\n",
    "                                    })\n",
    "\n",
    "# Model.Suffix, Workorder이 같다.\n",
    "df_train = df_train.drop(columns=['Model.Suffix_Fill1', 'Model.Suffix_Fill2', 'Model.Suffix_AutoClave'])\n",
    "df_train = df_train.drop(columns=['Workorder_Fill1', 'Workorder_Fill2', 'Workorder_AutoClave'])\n",
    "df_train = df_train.rename(columns={'Workorder_Dam': 'Workorder', 'Model.Suffix_Dam': 'Model.Suffix'})\n",
    "\n",
    "\n",
    "# 의미를 찾을 수 없는 컬럼들 제거\n",
    "df_train = df_train.drop(columns=['Wip Line_Fill1', \n",
    "                                  'Process Desc._Fill1', \n",
    "                                  'Insp. Seq No._Fill1', \n",
    "                                  'Insp Judge Code_Fill1', \n",
    "                                  'Equipment_AutoClave',\n",
    "                                  'Process Desc._AutoClave', \n",
    "                                  'Wip Line_AutoClave', \n",
    "                                  'Insp Judge Code_AutoClave',\n",
    "                                  'Insp. Seq No._AutoClave',\n",
    "                                  '1st Pressure Judge Value_AutoClave', \n",
    "                                  '2nd Pressure Judge Value_AutoClave', \n",
    "                                  '3rd Pressure Judge Value_AutoClave', \n",
    "                                  'GMES_ORIGIN_INSP_JUDGE_CODE Collect Result_AutoClave',\n",
    "                                  'GMES_ORIGIN_INSP_JUDGE_CODE Judge Value_AutoClave',\n",
    "                                  'GMES_ORIGIN_INSP_JUDGE_CODE Unit Time_AutoClave',\n",
    "                                  'Wip Line_Fill2', \n",
    "                                  'Process Desc._Fill2', \n",
    "                                  'Insp. Seq No._Fill2', \n",
    "                                  'Insp Judge Code_Fill2', \n",
    "                                  'Wip Line_Dam', \n",
    "                                  'Process Desc._Dam', \n",
    "                                  'Insp. Seq No._Dam', \n",
    "                                  'Insp Judge Code_Dam',\n",
    "                                  ])  \n",
    "\n",
    "# Fill2는 레진을 살포하지 않는다. UV만 진행하는 과정이므로 싹 삭제해 준다.          \n",
    "df_train = df_train.drop(columns=['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill2',\n",
    "                                'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill2',\n",
    "                                'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill2',\n",
    "                                'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill2',\n",
    "                                'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill2',\n",
    "                                'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill2',\n",
    "                                'HEAD NORMAL COORDINATE Z AXIS_Fill2',\n",
    "                                'HEAD Standby Position X Collect Result_Fill2',\n",
    "                                'HEAD Standby Position Y Collect Result_Fill2',\n",
    "                                'HEAD Standby Position Z Collect Result_Fill2',\n",
    "                                'Head Clean Position X Collect Result_Fill2',\n",
    "                                'Head Clean Position Y Collect Result_Fill2',\n",
    "                                'Head Clean Position Z Collect Result_Fill2',\n",
    "                                'Head Purge Position X Collect Result_Fill2',\n",
    "                                'Head Purge Position Y Collect Result_Fill2',\n",
    "                                'Head Purge Position Z Collect Result_Fill2',\n",
    "                                'DISCHARGED SPEED OF RESIN Collect Result_Fill2',\n",
    "                                'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill2',\n",
    "                                'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill2',\n",
    "                                'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill2',\n",
    "                                'Dispense Volume(Stage1) Collect Result_Fill2',\n",
    "                                'Dispense Volume(Stage2) Collect Result_Fill2',\n",
    "                                'Dispense Volume(Stage3) Collect Result_Fill2',])  \n",
    "\n",
    "# 라인별로 속도가 같은 경우, 다른 경우가 있다.\n",
    "df_train['Stage1 Line diffent Distance Speed_Dam'] = ((df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage1 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage1 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage1 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage1 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage1 Line3 Distance Speed Collect Result_Dam'] != df_train['Stage1 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "\n",
    "df_train['Stage2 Line diffent Distance Speed_Dam'] = ((df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage2 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage2 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage2 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage2 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage2 Line3 Distance Speed Collect Result_Dam'] != df_train['Stage2 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "\n",
    "df_train['Stage3 Line diffent Distance Speed_Dam'] = ((df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage3 Line2 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage3 Line3 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage3 Line1 Distance Speed Collect Result_Dam'] != df_train['Stage3 Line4 Distance Speed Collect Result_Dam']) |\n",
    "                                                  (df_train['Stage3 Line3 Distance Speed Collect Result_Dam'] != df_train['Stage3 Line4 Distance Speed Collect Result_Dam'])).astype(int)\n",
    "\n",
    "# 단일값이 하나인 컬럼들, 의미를 찾고싶다면 주석처리 해야하는 것들\n",
    "df_train = df_train.drop(columns=['CURE START POSITION X Collect Result_Dam', \n",
    "                                'CURE START POSITION Z Collect Result_Dam', \n",
    "                                'CURE START POSITION Θ Collect Result_Dam',\n",
    "                                'HEAD Standby Position X Collect Result_Dam',\n",
    "                                'HEAD Standby Position Y Collect Result_Dam',\n",
    "                                'HEAD Standby Position Z Collect Result_Dam',\n",
    "                                'Head Clean Position X Collect Result_Dam',\n",
    "                                'Head Clean Position Y Collect Result_Dam',\n",
    "                                'Head Purge Position X Collect Result_Dam',\n",
    "                                'Head Purge Position Y Collect Result_Dam',\n",
    "                                'Head Zero Position X Collect Result_Dam',\n",
    "                                'HEAD Standby Position X Collect Result_Fill1',\n",
    "                                'HEAD Standby Position Y Collect Result_Fill1',\n",
    "                                'HEAD Standby Position Z Collect Result_Fill1',\n",
    "                                'Head Clean Position X Collect Result_Fill1',\n",
    "                                'Head Clean Position Y Collect Result_Fill1',\n",
    "                                'Head Clean Position Z Collect Result_Fill1',\n",
    "                                'Head Purge Position X Collect Result_Fill1',\n",
    "                                'Head Purge Position Y Collect Result_Fill1',\n",
    "                                'CURE END POSITION X Collect Result_Fill2',\n",
    "                                'CURE END POSITION Θ Collect Result_Fill2',\n",
    "                                'CURE STANDBY POSITION X Collect Result_Fill2',\n",
    "                                'CURE STANDBY POSITION Z Collect Result_Fill2',\n",
    "                                'CURE STANDBY POSITION Θ Collect Result_Fill2',\n",
    "                                'CURE START POSITION X Collect Result_Fill2',\n",
    "                                'CURE START POSITION Θ Collect Result_Fill2',\n",
    "                                'CURE END POSITION X Collect Result_Dam',\n",
    "                                'CURE END POSITION Z Collect Result_Dam',\n",
    "                                'CURE END POSITION Θ Collect Result_Dam',\n",
    "                                'CURE STANDBY POSITION X Collect Result_Dam',\n",
    "                                'CURE STANDBY POSITION Z Collect Result_Dam',\n",
    "                                'CURE STANDBY POSITION Θ Collect Result_Dam',\n",
    "                                ])\n",
    "\n",
    "# df_train = df_train.drop(columns = ['HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
    "#                                    'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',\n",
    "#                                    ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "561aaa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 레이블 인코딩\n",
    "label_encoders = {}\n",
    "categorical_features = ['Model.Suffix','workorder_for', 'Chamber Temp. Judge Value_AutoClave']\n",
    "\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    df_train[feature] = le.fit_transform(df_train[feature])\n",
    "\n",
    "    df_test[feature] = le.transform(df_test[feature])\n",
    "    label_encoders[feature] = le"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288bb66b",
   "metadata": {},
   "source": [
    "## 5. Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d45ac99",
   "metadata": {},
   "source": [
    "### 선택할 칼럼 재 지정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "581d981c",
   "metadata": {},
   "outputs": [],
   "source": [
    "column = \\\n",
    "[\n",
    "#     'Temperature',\n",
    "#     'Humidity',\n",
    "'timediff_autoclave_fill2',\n",
    "# 'Year',\n",
    "'Month',\n",
    "# 'Day',\n",
    "'Hour_Dam',\n",
    "# 'Minute_Dam',\n",
    "'Hour_Fill1',\n",
    "# 'Minute_Fill1',\n",
    "'Hour_Fill2',\n",
    "# 'Minute_Fill2',\n",
    "'Hour_AutoClave',\n",
    "# 'Minute_AutoClave',   \n",
    "#  'Workorder',\n",
    " 'Production Qty',\n",
    " 'Receip No',  \n",
    " 'Model.Suffix',\n",
    " 'inconsistant',\n",
    " 'Equipment',\n",
    " 'PalletID',\n",
    "#  'workorder_first',\n",
    "#  'workorder_third',\n",
    "    'workorder_for',\n",
    " \n",
    " 'CURE SPEED Collect Result_Dam',\n",
    " 'DISCHARGED SPEED OF RESIN Collect Result_Dam', # 범주 가능\n",
    " \n",
    " 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam', # 10 미만이냐 아니냐.\n",
    " 'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam', # 4 미만이냐 아니냐.\n",
    " 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam', # 10 미만이냐 아니냐\n",
    " 'Dispense Volume(Stage1) Collect Result_Dam',\n",
    " 'Dispense Volume(Stage2) Collect Result_Dam', # 0.3미만이냐 아니냐\n",
    " 'Dispense Volume(Stage3) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS_Dam',\n",
    " 'Head Clean Position Z Collect Result_Dam',\n",
    " 'Head Purge Position Z Collect Result_Dam',\n",
    " 'Head Zero Position Y Collect Result_Dam',\n",
    " 'Head Zero Position Z Collect Result_Dam',\n",
    " 'Machine Tact time Collect Result_Dam',\n",
    " 'Stage1 Circle Distance Speed_Dam',\n",
    "#  'Stage1 Line1 Distance Speed Collect Result_Dam',\n",
    "#  'Stage1 Line2 Distance Speed Collect Result_Dam',\n",
    "#  'Stage1 Line3 Distance Speed Collect Result_Dam',\n",
    "#  'Stage1 Line4 Distance Speed Collect Result_Dam',\n",
    " 'Stage2 Circle Distance Speed_Dam',\n",
    "#  'Stage2 Line1 Distance Speed Collect Result_Dam',\n",
    "#  'Stage2 Line2 Distance Speed Collect Result_Dam',\n",
    "#  'Stage2 Line3 Distance Speed Collect Result_Dam',\n",
    "#  'Stage2 Line4 Distance Speed Collect Result_Dam',\n",
    " 'Stage3 Circle Distance Speed_Dam',\n",
    "#  'Stage3 Line1 Distance Speed Collect Result_Dam',\n",
    "#  'Stage3 Line2 Distance Speed Collect Result_Dam',\n",
    "#  'Stage3 Line3 Distance Speed Collect Result_Dam',\n",
    "#  'Stage3 Line4 Distance Speed Collect Result_Dam',\n",
    " 'THICKNESS 1 Collect Result_Dam',\n",
    " 'THICKNESS 2 Collect Result_Dam',\n",
    " 'THICKNESS 3 Collect Result_Dam',\n",
    " \n",
    " 'DISCHARGED SPEED OF RESIN Collect Result_Fill1', # 범주 가능\n",
    " 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1', # 17.4 이상이냐 아니냐\n",
    " 'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1', # 5 초과냐 아니냐\n",
    " 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1', # 17 이상이냐 아니냐\n",
    " 'Dispense Volume(Stage1) Collect Result_Fill1', # 12 미만이냐 아니냐\n",
    " 'Dispense Volume(Stage2) Collect Result_Fill1', # 4.5 초과냐 아니냐\n",
    " 'Dispense Volume(Stage3) Collect Result_Fill1', # 12 미만이냐 아니냐\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',\n",
    " 'Head Purge Position Z Collect Result_Fill1',\n",
    " 'Machine Tact time Collect Result_Fill1',\n",
    " \n",
    " 'CURE END POSITION Z Collect Result_Fill2',\n",
    " 'CURE SPEED Collect Result_Fill2',\n",
    " 'CURE START POSITION Z Collect Result_Fill2',\n",
    " 'Machine Tact time Collect Result_Fill2',\n",
    " '1st Pressure Collect Result_AutoClave',\n",
    " '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
    " '2nd Pressure Collect Result_AutoClave',\n",
    " '2nd Pressure Unit Time_AutoClave',\n",
    " '3rd Pressure Collect Result_AutoClave',\n",
    " '3rd Pressure Unit Time_AutoClave',\n",
    " 'Chamber Temp. Collect Result_AutoClave',\n",
    " 'Chamber Temp. Unit Time_AutoClave',\n",
    " 'Chamber Temp. Judge Value_AutoClave',\n",
    "#  '1st_Pressure_PCA',\n",
    "#  '2nd_Pressure_PCA',\n",
    "#  '3rd_Pressure_PCA',\n",
    " \n",
    "#  'Minus1_Dam',\n",
    "#  'Minus2_Dam',\n",
    " 'Minus1Y_Dam',\n",
    " 'Minus2Y_Dam',\n",
    "#  'HEAD NORMAL COORDINATE Error (Stage1)_Fill1',\n",
    "#  'HEAD NORMAL COORDINATE Error (Stage2)_Fill1',\n",
    "#  'HEAD NORMAL COORDINATE Error (Stage3)_Fill1',\n",
    "#  'HEAD NORMAL COORDINATE Error (Stage1)_Dam',\n",
    "#  'HEAD NORMAL COORDINATE Error (Stage2)_Dam',\n",
    "#  'HEAD NORMAL COORDINATE Error (Stage3)_Dam',\n",
    "#  '1st Pressure Power_AutoClave',\n",
    "#  '2nd Pressure Power_AutoClave',\n",
    "#  '3rd Pressure Power_AutoClave',\n",
    "#  '1st Power x Temp_AutoCLave',\n",
    "#  '2nd Power x Temp_AutoCLave',\n",
    "#  '3rd Power x Temp_AutoCLave',\n",
    " 'Stage1 Line Sum Speed_Dam',\n",
    " 'Stage2 Line Sum Speed_Dam',\n",
    " 'Stage3 Line Sum Speed_Dam',\n",
    "#  '1st Pressure x Time x Temp AutoClave',\n",
    "#  '2nd Pressure x Time x Temp AutoClave',\n",
    "#  '3rd Pressure x Time x Temp AutoClave',\n",
    "#  'RESIN Predicted_Volume Stage1 Dam',\n",
    "#  'RESIN Predicted_Volume Stage2 Dam',\n",
    "#  'RESIN Predicted_Volume Stage3 Dam',\n",
    "#  'Stage1 Scaling_Factor',\n",
    "#  'Stage2 Scaling_Factor',\n",
    "#  'Stage3 Scaling_Factor',\n",
    "#  'RESIN Adjusted_Predicted_Volume Stage1 Dam',\n",
    "#  'RESIN Adjusted_Predicted_Volume Stage2 Dam',\n",
    "#  'RESIN Adjusted_Predicted_Volume Stage3 Dam',\n",
    "#  'RESIN Predicted_Volume Stage1 Fill1',\n",
    "#  'RESIN Predicted_Volume Stage2 Fill1',\n",
    "#  'RESIN Predicted_Volume Stage3 Fill1',\n",
    "#  'RESIN Adjusted_Predicted_Volume Stage1 Fill1',\n",
    "#  'RESIN Adjusted_Predicted_Volume Stage2 Fill1',\n",
    "#  'RESIN Adjusted_Predicted_Volume Stage3 Fill1',\n",
    " 'Stage1 Line diffent Distance Speed_Dam',\n",
    " 'Stage2 Line diffent Distance Speed_Dam',\n",
    " 'Stage3 Line diffent Distance Speed_Dam',\n",
    "#  'kmeans',\n",
    " \n",
    " 'target']\n",
    "\n",
    "\n",
    "\n",
    "column_t = [\n",
    "    \n",
    "#     'Temperature',\n",
    "#     'Humidity',\n",
    "    \n",
    "'timediff_autoclave_fill2',\n",
    "# 'Year',\n",
    "'Month',\n",
    "# 'Day',\n",
    "'Hour_Dam',\n",
    "# 'Minute_Dam',\n",
    "'Hour_Fill1',\n",
    "# 'Minute_Fill1',\n",
    "'Hour_Fill2',\n",
    "# 'Minute_Fill2',\n",
    "'Hour_AutoClave',\n",
    "# 'Minute_AutoClave',\n",
    "#  'Workorder',\n",
    "\n",
    " 'Production Qty',\n",
    " 'Receip No',  \n",
    " 'Model.Suffix',\n",
    " 'inconsistant',\n",
    " 'Equipment',\n",
    " 'PalletID',\n",
    "#  'workorder_first',\n",
    "#  'workorder_third',\n",
    "    'workorder_for',\n",
    " \n",
    " 'CURE SPEED Collect Result_Dam',\n",
    " 'DISCHARGED SPEED OF RESIN Collect Result_Dam',\n",
    " 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Dam',\n",
    " 'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Dam',\n",
    " 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Dam',\n",
    " 'Dispense Volume(Stage1) Collect Result_Dam',\n",
    " 'Dispense Volume(Stage2) Collect Result_Dam',\n",
    " 'Dispense Volume(Stage3) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Dam',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS_Dam',\n",
    " 'Head Clean Position Z Collect Result_Dam',\n",
    " 'Head Purge Position Z Collect Result_Dam',\n",
    " 'Head Zero Position Y Collect Result_Dam',\n",
    " 'Head Zero Position Z Collect Result_Dam',\n",
    " 'Machine Tact time Collect Result_Dam',\n",
    " 'Stage1 Circle Distance Speed_Dam',\n",
    "#  'Stage1 Line1 Distance Speed Collect Result_Dam',\n",
    "#  'Stage1 Line2 Distance Speed Collect Result_Dam',\n",
    "#  'Stage1 Line3 Distance Speed Collect Result_Dam',\n",
    "#  'Stage1 Line4 Distance Speed Collect Result_Dam',\n",
    " 'Stage2 Circle Distance Speed_Dam',\n",
    "#  'Stage2 Line1 Distance Speed Collect Result_Dam',\n",
    "#  'Stage2 Line2 Distance Speed Collect Result_Dam',\n",
    "#  'Stage2 Line3 Distance Speed Collect Result_Dam',\n",
    "#  'Stage2 Line4 Distance Speed Collect Result_Dam',\n",
    " 'Stage3 Circle Distance Speed_Dam',\n",
    "#  'Stage3 Line1 Distance Speed Collect Result_Dam',\n",
    "#  'Stage3 Line2 Distance Speed Collect Result_Dam',\n",
    "#  'Stage3 Line3 Distance Speed Collect Result_Dam',\n",
    "#  'Stage3 Line4 Distance Speed Collect Result_Dam',\n",
    " 'THICKNESS 1 Collect Result_Dam',\n",
    " 'THICKNESS 2 Collect Result_Dam',\n",
    " 'THICKNESS 3 Collect Result_Dam',\n",
    " \n",
    " 'DISCHARGED SPEED OF RESIN Collect Result_Fill1',\n",
    " 'DISCHARGED TIME OF RESIN(Stage1) Collect Result_Fill1',\n",
    " 'DISCHARGED TIME OF RESIN(Stage2) Collect Result_Fill1',\n",
    " 'DISCHARGED TIME OF RESIN(Stage3) Collect Result_Fill1',\n",
    " 'Dispense Volume(Stage1) Collect Result_Fill1',\n",
    " 'Dispense Volume(Stage2) Collect Result_Fill1',\n",
    " 'Dispense Volume(Stage3) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage1) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage2) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE X AXIS(Stage3) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage1) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage2) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Y AXIS(Stage3) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage1) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage2) Collect Result_Fill1',\n",
    " 'HEAD NORMAL COORDINATE Z AXIS(Stage3) Collect Result_Fill1',\n",
    " 'Head Purge Position Z Collect Result_Fill1',\n",
    " 'Machine Tact time Collect Result_Fill1',\n",
    " \n",
    " 'CURE END POSITION Z Collect Result_Fill2',\n",
    " 'CURE SPEED Collect Result_Fill2',\n",
    " 'CURE START POSITION Z Collect Result_Fill2',\n",
    " 'Machine Tact time Collect Result_Fill2',\n",
    " '1st Pressure Collect Result_AutoClave',\n",
    " '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
    " '2nd Pressure Collect Result_AutoClave',\n",
    " '2nd Pressure Unit Time_AutoClave',\n",
    " '3rd Pressure Collect Result_AutoClave',\n",
    " '3rd Pressure Unit Time_AutoClave',\n",
    " 'Chamber Temp. Collect Result_AutoClave',\n",
    " 'Chamber Temp. Unit Time_AutoClave',\n",
    " 'Chamber Temp. Judge Value_AutoClave',\n",
    "#  '1st_Pressure_PCA',\n",
    "#  '2nd_Pressure_PCA',\n",
    "#  '3rd_Pressure_PCA',\n",
    "#  'Minus1_Dam',\n",
    "#  'Minus2_Dam',\n",
    " 'Minus1Y_Dam',\n",
    " 'Minus2Y_Dam',\n",
    "#  'HEAD NORMAL COORDINATE Error (Stage1)_Fill1',\n",
    "#  'HEAD NORMAL COORDINATE Error (Stage2)_Fill1',\n",
    "#  'HEAD NORMAL COORDINATE Error (Stage3)_Fill1',\n",
    "#  'HEAD NORMAL COORDINATE Error (Stage1)_Dam',\n",
    "#  'HEAD NORMAL COORDINATE Error (Stage2)_Dam',\n",
    "#  'HEAD NORMAL COORDINATE Error (Stage3)_Dam',\n",
    "#  '1st Pressure Power_AutoClave',\n",
    "#  '2nd Pressure Power_AutoClave',\n",
    "#  '3rd Pressure Power_AutoClave',\n",
    "#  '1st Power x Temp_AutoCLave',\n",
    "#  '2nd Power x Temp_AutoCLave',\n",
    "#  '3rd Power x Temp_AutoCLave',\n",
    " 'Stage1 Line Sum Speed_Dam',\n",
    " 'Stage2 Line Sum Speed_Dam',\n",
    " 'Stage3 Line Sum Speed_Dam',\n",
    "#  '1st Pressure x Time x Temp AutoClave',\n",
    "#  '2nd Pressure x Time x Temp AutoClave',\n",
    "#  '3rd Pressure x Time x Temp AutoClave',\n",
    "#  'RESIN Predicted_Volume Stage1 Dam',\n",
    "#  'RESIN Predicted_Volume Stage2 Dam',\n",
    "#  'RESIN Predicted_Volume Stage3 Dam',\n",
    "#  'Stage1 Scaling_Factor',\n",
    "#  'Stage2 Scaling_Factor',\n",
    "#  'Stage3 Scaling_Factor',\n",
    "#  'RESIN Adjusted_Predicted_Volume Stage1 Dam',\n",
    "#  'RESIN Adjusted_Predicted_Volume Stage2 Dam',\n",
    "#  'RESIN Adjusted_Predicted_Volume Stage3 Dam',\n",
    "#  'RESIN Predicted_Volume Stage1 Fill1',\n",
    "#  'RESIN Predicted_Volume Stage2 Fill1',\n",
    "#  'RESIN Predicted_Volume Stage3 Fill1',\n",
    "#  'RESIN Adjusted_Predicted_Volume Stage1 Fill1',\n",
    "#  'RESIN Adjusted_Predicted_Volume Stage2 Fill1',\n",
    "#  'RESIN Adjusted_Predicted_Volume Stage3 Fill1',\n",
    " 'Stage1 Line diffent Distance Speed_Dam',\n",
    " 'Stage2 Line diffent Distance Speed_Dam',\n",
    " 'Stage3 Line diffent Distance Speed_Dam',\n",
    "#             'kmeans',\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d59d8f5",
   "metadata": {},
   "source": [
    "### 각 모델별 선정한 칼럼 데이터 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "id": "6cd2878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_train = df_train[column].copy()\n",
    "cat_test = df_test[column_t].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "id": "0618c598",
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbm_train = df_train[column].copy()\n",
    "lgbm_test = df_test[column_t].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "6afa5d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_train = df_train[column].copy()\n",
    "xgb_test = df_test[column_t].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "855179d0",
   "metadata": {},
   "source": [
    "### 범주형 변수 선택 및 전환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "id": "ae02c8ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_convert = ['Equipment', 'PalletID', 'Model.Suffix', 'Receip No']# 'Workorder']\n",
    "columns_to = [\n",
    "    \n",
    "# 'Year',\n",
    "'Month',\n",
    "# 'Day',\n",
    "'Hour_Dam',\n",
    "# 'Minute_Dam',\n",
    "'Hour_Fill1',\n",
    "# 'Minute_Fill1',\n",
    "'Hour_Fill2',\n",
    "# 'Minute_Fill2',\n",
    "'Hour_AutoClave',\n",
    "# 'Minute_AutoClave',\n",
    "              'Head Zero Position Y Collect Result_Dam',\n",
    "              'Head Zero Position Z Collect Result_Dam',\n",
    "              'Head Clean Position Z Collect Result_Dam',\n",
    "              'Head Purge Position Z Collect Result_Dam',\n",
    "              'Head Purge Position Z Collect Result_Fill1',\n",
    "              'CURE START POSITION Z Collect Result_Fill2',\n",
    "              'CURE END POSITION Z Collect Result_Fill2',\n",
    "              'CURE SPEED Collect Result_Fill2',\n",
    "              'Stage1 Circle Distance Speed_Dam',\n",
    "              'Stage2 Circle Distance Speed_Dam',\n",
    "              'Stage3 Circle Distance Speed_Dam',\n",
    "              'Stage1 Line diffent Distance Speed_Dam',\n",
    "              'Stage2 Line diffent Distance Speed_Dam',\n",
    "              'Stage3 Line diffent Distance Speed_Dam',\n",
    "              'Minus1Y_Dam', \n",
    "              'Minus2Y_Dam', \n",
    "              'inconsistant',\n",
    "              'HEAD NORMAL COORDINATE Z AXIS_Dam',\n",
    "              '1st Pressure 1st Pressure Unit Time_AutoClave',\n",
    "              '2nd Pressure Unit Time_AutoClave',\n",
    "              '3rd Pressure Unit Time_AutoClave',\n",
    "              'Chamber Temp. Judge Value_AutoClave',\n",
    "              'workorder_for'\n",
    "             ]\n",
    "\n",
    "dtype = 'string'  # 원하는 데이터 타입\n",
    "for column in columns_to_convert + columns_to:\n",
    "    cat_train[column] = cat_train[column].astype(dtype)\n",
    "    cat_test[column] = cat_test[column].astype(dtype)\n",
    "    \n",
    "dtype = 'category'  # 원하는 데이터 타입\n",
    "for column in columns_to_convert + columns_to:\n",
    "    cat_train[column] = cat_train[column].astype(dtype)\n",
    "    cat_test[column] = cat_test[column].astype(dtype)\n",
    "    \n",
    "dtype = 'float'  # 원하는 데이터 타입\n",
    "for column in columns_to_convert + columns_to:\n",
    "    lgbm_train[column] = lgbm_train[column].astype(dtype)\n",
    "    lgbm_test[column] = lgbm_test[column].astype(dtype)\n",
    "    \n",
    "dtype = 'category'  # 원하는 데이터 타입\n",
    "for column in columns_to_convert + columns_to:\n",
    "    lgbm_train[column] = lgbm_train[column].astype(dtype)\n",
    "    lgbm_test[column] = lgbm_test[column].astype(dtype)\n",
    "    \n",
    "dtype = 'float'  # 원하는 데이터 타입\n",
    "for column in columns_to_convert + columns_to:\n",
    "    xgb_train[column] = xgb_train[column].astype(dtype)\n",
    "    xgb_test[column] = xgb_test[column].astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a631d5",
   "metadata": {},
   "source": [
    "### 모델링 추정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7342db59",
   "metadata": {},
   "source": [
    "#### Catboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "1191af8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x7f68a16eaa10>"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = cat_train.drop(columns=['target'])\n",
    "y_train = cat_train['target'].apply(lambda x: True if x == 'AbNormal' else False)\n",
    "\n",
    "cat_features_indices = columns_to_convert + columns_to\n",
    "\n",
    "# 최적의 하이퍼파라미터로 모델 재학습\n",
    "cat_best_params = {'iterations': 246, 'depth': 9, 'learning_rate': 0.025081156860452335} # study.best_trial.params\n",
    "cat_best_params[\"random_seed\"] = 42\n",
    "cat_best_params['verbose'] = 0\n",
    "cat_best_model = CatBoostClassifier(**cat_best_params)\n",
    "cat_best_model.fit(X_train, y_train, cat_features=cat_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "34d635d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict proba 구하기\n",
    "X_valid = cat_test\n",
    "y_pred_proba = cat_best_model.predict_proba(X_valid)[:, 1]\n",
    "proba1 = y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daab827f",
   "metadata": {},
   "source": [
    "#### LightGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "fdc8bc6e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;background-color: white;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LGBMClassifier(learning_rate=0.04149455426040617, max_depth=31,\n",
       "               n_estimators=904, random_seed=42, random_state=42, verbose=-1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LGBMClassifier</label><div class=\"sk-toggleable__content\"><pre>LGBMClassifier(learning_rate=0.04149455426040617, max_depth=31,\n",
       "               n_estimators=904, random_seed=42, random_state=42, verbose=-1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LGBMClassifier(learning_rate=0.04149455426040617, max_depth=31,\n",
       "               n_estimators=904, random_seed=42, random_state=42, verbose=-1)"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optuna 최적 파라미터로 학습\n",
    "X_train = lgbm_train.drop(columns=['target'])\n",
    "y_train = lgbm_train['target'].apply(lambda x: True if x == 'AbNormal' else False)\n",
    "\n",
    "lgbm_best_params = {'n_estimators': 904, 'max_depth': 31, 'learning_rate': 0.04149455426040617}\n",
    "lgbm_best_params[\"random_seed\"] = 42\n",
    "lgbm_best_params[\"random_state\"] = 42\n",
    "lgbm_best_params['verbose'] = -1\n",
    "\n",
    "lgbm_best_model = LGBMClassifier(**lgbm_best_params)\n",
    "lgbm_best_model.fit(X_train, y_train, categorical_feature=cat_features_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "f417e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# proba \n",
    "X_valid = lgbm_test\n",
    "y_pred_proba = lgbm_best_model.predict_proba(X_valid)[:, 1]\n",
    "proba2 = y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0e0fff",
   "metadata": {},
   "source": [
    "#### XGboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "8038f70d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-14 {color: black;background-color: white;}#sk-container-id-14 pre{padding: 0;}#sk-container-id-14 div.sk-toggleable {background-color: white;}#sk-container-id-14 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-14 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-14 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-14 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-14 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-14 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-14 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-14 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-14 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-14 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-14 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-14 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-14 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-14 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-14 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-14 div.sk-item {position: relative;z-index: 1;}#sk-container-id-14 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-14 div.sk-item::before, #sk-container-id-14 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-14 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-14 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-14 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-14 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-14 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-14 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-14 div.sk-label-container {text-align: center;}#sk-container-id-14 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-14 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-14\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.008215675096193411,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=365, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" checked><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.008215675096193411,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=365, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=None, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False, eval_metric=None, feature_types=None,\n",
       "              gamma=None, grow_policy=None, importance_type=None,\n",
       "              interaction_constraints=None, learning_rate=0.008215675096193411,\n",
       "              max_bin=None, max_cat_threshold=None, max_cat_to_onehot=None,\n",
       "              max_delta_step=None, max_depth=10, max_leaves=None,\n",
       "              min_child_weight=None, missing=nan, monotone_constraints=None,\n",
       "              multi_strategy=None, n_estimators=365, n_jobs=None,\n",
       "              num_parallel_tree=None, random_state=42, ...)"
      ]
     },
     "execution_count": 446,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최적 파라미터로 학습\n",
    "X_train = xgb_train.drop(columns=['target'])\n",
    "y_train = xgb_train['target'].apply(lambda x: True if x == 'AbNormal' else False)\n",
    "\n",
    "xgb_best_params =  {'n_estimators': 365, 'max_depth': 10, 'learning_rate': 0.008215675096193411} # {'n_estimators': 365, 'max_depth': 10, 'learning_rate': 0.008215675096193411} # study.best_trial.params\n",
    "xgb_best_params[\"random_state\"] = 42\n",
    "xgb_best_params[\"seed\"] = 42\n",
    "\n",
    "xgb_best_model = xgb.XGBClassifier(**xgb_best_params)\n",
    "xgb_best_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "7832ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid = xgb_test\n",
    "y_pred_proba = xgb_best_model.predict_proba(X_valid)[:, 1]\n",
    "proba3 = y_pred_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "f4da21ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid search로 찾은 weight 대입하여 threshold 적용\n",
    "y_best = (14 * proba1 + 2 * proba2 + 3 * proba3) / (19)\n",
    "y_pred_custom_threshold = (y_best >= 0.1339928075297637).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "499bd7ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "374"
      ]
     },
     "execution_count": 449,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 최종 이상치 예측 개수 확인\n",
    "sum(y_pred_custom_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb83eca",
   "metadata": {},
   "source": [
    "## 6. Inference & Submission\n",
    "\n",
    "### 제출코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "af9b0b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 민감성 코드 - 하나라도 이상치라고 반영하면 무조건 이상치로 만들기\n",
    "y_pred = np.where((y_pred_custom_threshold) == 0, \"Normal\", \"AbNormal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "d2c847cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# submission\n",
    "df_sub = pd.read_csv('submission.csv')\n",
    "df_sub[\"target\"] = y_pred\n",
    "\n",
    "# 제출 파일 저장\n",
    "df_sub.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
